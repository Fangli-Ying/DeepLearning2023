
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformer model for language understanding （未完待续） &#8212; 深度学习</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/chpt7/transformer';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/dp.gif" class="logo__image only-light" alt="深度学习 - Home"/>
    <img src="../../_static/dp.gif" class="logo__image only-dark pst-js-only" alt="深度学习 - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    欢迎来到《深度学习原理及其应用》
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">深度学习基础</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt1/DeepLearning.html">第一章 深度学习基础</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">视觉基本任务</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch1-Object-Detection.html">1. 目标检测简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch2-EDA.html">2. 数据探索</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch4-RetinaNet.html">4. RetinaNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch5-Faster-R-CNN.html">5. Faster R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch5-References.html">6. 참고 문헌</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">深度生成模型</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch1-Introduction.html">1. 生成对抗网络（GAN）简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch2-EDA.html">2. EDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch3-GAN.html">3. GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch4-pix2pix.html">4. pix2pix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch5-VAE.html">Variational Autoencoder（未完成）</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">序列模型</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch1-Time-Series.html">1. Time Series 时间序列简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch2-EDA.html">2. EDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch3-preprocessing.html">3. 数据预处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch4-LSTM.html">4. LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch5-CNN-LSTM.html">5. CNN-LSTM</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">自然语言处理</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt5/Ch1-Introduction.html">1. NLP模型简介 (Introduction to NLP Model)（未完待续）</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">图表示学习</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt6/graph.html">第六章 图理论基础 （未完待续）</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/Fangli-Ying/DeepLearning2023/master?urlpath=tree/chapters/chpt7/transformer.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://jupyter.org/hub/hub/user-redirect/git-pull?repo=https%3A//github.com/Fangli-Ying/DeepLearning2023&urlpath=tree/DeepLearning2023/chapters/chpt7/transformer.ipynb&branch=master" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/Fangli-Ying/DeepLearning2023/blob/master/chapters/chpt7/transformer.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/chpt7/transformer.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformer model for language understanding （未完待续）</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-input-pipeline">Setup input pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masking">Masking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled dot product attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-wise-feed-forward-network">Point wise feed forward network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-and-decoder">Encoder and decoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-layer">Encoder layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-layer">Decoder layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-transformer">Create the Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-hyperparameters">Set hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-and-metrics">Loss and metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-checkpointing">Training and checkpointing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate">Evaluate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformer-model-for-language-understanding">
<h1>Transformer model for language understanding （未完待续）<a class="headerlink" href="#transformer-model-for-language-understanding" title="Link to this heading">#</a></h1>
<p>This tutorial trains a <a href="https://arxiv.org/abs/1706.03762" class="external">Transformer model</a> to translate Portuguese to English. This is an advanced example that assumes knowledge of <a class="reference internal" href="#text_generation.ipynb"><span class="xref myst">text generation</span></a> and <a class="reference internal" href="#nmt_with_attention.ipynb"><span class="xref myst">attention</span></a>.</p>
<p>The core idea behind the Transformer model is <em>self-attention</em>—the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections <em>Scaled dot product attention</em> and <em>Multi-head attention</em>.</p>
<p>A transformer model handles variable-sized input using stacks of self-attention layers instead of <a class="reference internal" href="#text_classification_rnn.ipynb"><span class="xref myst">RNNs</span></a> or <a class="reference internal" href="#../images/intro_to_cnns.ipynb"><span class="xref myst">CNNs</span></a>. This general architecture has a number of advantages:</p>
<ul class="simple">
<li><p>It make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, <a class="reference external" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8">StarCraft units</a>).</p></li>
<li><p>Layer outputs can be calculated in parallel, instead of a series like an RNN.</p></li>
<li><p>Distant items can affect each other’s output without passing through many RNN-steps, or convolution layers (see <a class="reference external" href="https://arxiv.org/pdf/1903.03878.pdf">Scene Memory Transformer</a> for example).</p></li>
<li><p>It can learn long-range dependencies. This is a challenge in many sequence tasks.</p></li>
</ul>
<p>The downsides of this architecture are:</p>
<ul class="simple">
<li><p>For a time-series, the output for a time-step is calculated from the <em>entire history</em> instead of only the inputs and current hidden-state. This <em>may</em> be less efficient.</p></li>
<li><p>If the input <em>does</em> have a  temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words.</p></li>
</ul>
<p>After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.</p>
<img src="https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png" width="800" alt="Attention heatmap"><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-q<span class="w"> </span>tfds-nightly

<span class="c1"># Pin matplotlib version to 3.2.2 since in the latest version</span>
<span class="c1"># transformer.ipynb fails with the following error:</span>
<span class="c1"># https://stackoverflow.com/questions/62953704/valueerror-the-number-of-fixedlocator-locations-5-usually-from-a-call-to-set</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">matplotlib</span><span class="o">==</span><span class="m">3</span>.2.2
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ERROR: Exception:
Traceback (most recent call last):
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\cli\base_command.py&quot;, line 173, in _main
    status = self.run(options, args)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\cli\req_command.py&quot;, line 203, in wrapper
    return func(self, options, args)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\commands\install.py&quot;, line 315, in run
    requirement_set = resolver.resolve(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\resolution\resolvelib\resolver.py&quot;, line 94, in resolve
    result = self._result = resolver.resolve(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py&quot;, line 472, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py&quot;, line 341, in resolve
    self._add_to_criteria(self.state.criteria, r, parent=None)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\resolvelib\resolvers.py&quot;, line 172, in _add_to_criteria
    if not criterion.candidates:
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\resolvelib\structs.py&quot;, line 151, in __bool__
    return bool(self._sequence)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py&quot;, line 140, in __bool__
    return any(self)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py&quot;, line 128, in &lt;genexpr&gt;
    return (c for c in iterator if id(c) not in self._incompatible_ids)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\resolution\resolvelib\found_candidates.py&quot;, line 29, in _iter_built
    for version, func in infos:
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\resolution\resolvelib\factory.py&quot;, line 272, in iter_index_candidate_infos
    result = self._finder.find_best_candidate(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\package_finder.py&quot;, line 851, in find_best_candidate
    candidates = self.find_all_candidates(project_name)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\package_finder.py&quot;, line 798, in find_all_candidates
    page_candidates = list(page_candidates_it)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\sources.py&quot;, line 134, in page_candidates
    yield from self._candidates_from_page(self._link)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\package_finder.py&quot;, line 758, in process_project_url
    html_page = self._link_collector.fetch_page(project_url)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\collector.py&quot;, line 490, in fetch_page
    return _get_html_page(location, session=self.session)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\collector.py&quot;, line 400, in _get_html_page
    resp = _get_html_response(url, session=session)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\index\collector.py&quot;, line 115, in _get_html_response
    resp = session.get(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\requests\sessions.py&quot;, line 555, in get
    return self.request(&#39;GET&#39;, url, **kwargs)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_internal\network\session.py&quot;, line 454, in request
    return super().request(method, url, *args, **kwargs)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\requests\sessions.py&quot;, line 542, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\requests\sessions.py&quot;, line 655, in send
    r = adapter.send(request, **kwargs)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\cachecontrol\adapter.py&quot;, line 53, in send
    resp = super(CacheControlAdapter, self).send(request, **kw)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\requests\adapters.py&quot;, line 439, in send
    resp = conn.urlopen(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\connectionpool.py&quot;, line 696, in urlopen
    self._prepare_proxy(conn)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\connectionpool.py&quot;, line 964, in _prepare_proxy
    conn.connect()
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\connection.py&quot;, line 359, in connect
    conn = self._connect_tls_proxy(hostname, conn)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\connection.py&quot;, line 500, in _connect_tls_proxy
    return ssl_wrap_socket(
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\util\ssl_.py&quot;, line 453, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
  File &quot;D:\Program Files\Python39\lib\site-packages\pip\_vendor\urllib3\util\ssl_.py&quot;, line 495, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock)
  File &quot;D:\Program Files\Python39\lib\ssl.py&quot;, line 500, in wrap_socket
    return self.sslsocket_class._create(
  File &quot;D:\Program Files\Python39\lib\ssl.py&quot;, line 997, in _create
    raise ValueError(&quot;check_hostname requires server_hostname&quot;)
ValueError: check_hostname requires server_hostname
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">time</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;tensorflow&#39;
</pre></div>
</div>
</div>
</div>
<section id="setup-input-pipeline">
<h2>Setup input pipeline<a class="headerlink" href="#setup-input-pipeline" title="Link to this heading">#</a></h2>
<p>Use <a class="reference external" href="https://www.tensorflow.org/datasets">TFDS</a> to load the <a class="reference external" href="https://github.com/neulab/word-embeddings-for-nmt">Portugese-English translation dataset</a> from the <a class="reference external" href="https://www.ted.com/participate/translate">TED Talks Open Translation Project</a>.</p>
<p>This dataset contains approximately 50000 training examples, 1100 validation examples, and 2000 test examples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">examples</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;ted_hrlr_translate/pt_to_en&#39;</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Downloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...</span>
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5058c52cdc8b40caa21d3081d77176ee", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "db417f97c0de47aab8ac1cfd48fc9576", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "7c3f451a781a43dfb87adfb40172ff2f", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "977c5b6d1a0048508fc6a28dc2d695ed", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJX0UIR/ted_hrlr_translate-train.tfrecord
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "caa274a1a49343bbb35c139b2fc8cb5e", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "bef4db3a3023455fbb7da6dce721bbe7", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJX0UIR/ted_hrlr_translate-validation.tfrecord
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "402bf361ef5b4bf19d8bd9ab1a543474", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a86dbe7b8d8d42409a07972bfdc3776d", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteJX0UIR/ted_hrlr_translate-test.tfrecord
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "00d965907ea54f548f2cbf7b6d87aafb", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">Dataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.</span>
</pre></div>
</div>
</div>
</div>
<p>Create a custom subwords tokenizer from the training dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer_en</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>

<span class="n">tokenizer_pt</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">SubwordTextEncoder</span><span class="o">.</span><span class="n">build_from_corpus</span><span class="p">(</span>
    <span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">pt</span><span class="p">,</span> <span class="n">en</span> <span class="ow">in</span> <span class="n">train_examples</span><span class="p">),</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">13</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_string</span> <span class="o">=</span> <span class="s1">&#39;Transformer is awesome.&#39;</span>

<span class="n">tokenized_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Tokenized string is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">))</span>

<span class="n">original_string</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_string</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The original string: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_string</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">original_string</span> <span class="o">==</span> <span class="n">sample_string</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]
The original string: Transformer is awesome.
</pre></div>
</div>
</div>
</div>
<p>The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">tokenized_string</span><span class="p">:</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> ----&gt; </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">ts</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7915 ----&gt; T
1248 ----&gt; ran
7946 ----&gt; s
7194 ----&gt; former 
13 ----&gt; is 
2799 ----&gt; awesome
7877 ----&gt; .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
</pre></div>
</div>
</div>
</div>
<p>Add a start and end token to the input and target.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span><span class="p">):</span>
  <span class="n">lang1</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang1</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">lang2</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
      <span class="n">lang2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">lang1</span><span class="p">,</span> <span class="n">lang2</span>
</pre></div>
</div>
</div>
</div>
<p>You want to use <code class="docutils literal notranslate"><span class="pre">Dataset.map</span></code> to apply this function to each element of the dataset.  <code class="docutils literal notranslate"><span class="pre">Dataset.map</span></code> runs in graph mode.</p>
<ul class="simple">
<li><p>Graph tensors do not have a value.</p></li>
<li><p>In graph mode you can only use TensorFlow Ops and functions.</p></li>
</ul>
<p>So you can’t <code class="docutils literal notranslate"><span class="pre">.map</span></code> this function directly: You need to wrap it in a <code class="docutils literal notranslate"><span class="pre">tf.py_function</span></code>. The <code class="docutils literal notranslate"><span class="pre">tf.py_function</span></code> will pass regular tensors (with a value and a <code class="docutils literal notranslate"><span class="pre">.numpy()</span></code> method to access it), to the wrapped python function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tf_encode</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">):</span>
  <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_function</span><span class="p">(</span><span class="n">encode</span><span class="p">,</span> <span class="p">[</span><span class="n">pt</span><span class="p">,</span> <span class="n">en</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">])</span>
  <span class="n">result_pt</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">])</span>
  <span class="n">result_en</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="kc">None</span><span class="p">])</span>

  <span class="k">return</span> <span class="n">result_pt</span><span class="p">,</span> <span class="n">result_en</span>
</pre></div>
</div>
</div>
</div>
<p>Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">40</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">filter_max_length</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">,</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span>
<span class="c1"># cache the dataset to memory to get a speedup while reading from it.</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>


<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_examples</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tf_encode</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">filter_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))</span>
<span class="n">pt_batch</span><span class="p">,</span> <span class="n">en_batch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;tf.Tensor: shape=(64, 38), dtype=int64, numpy=
 array([[8214,  342, 3032, ...,    0,    0,    0],
        [8214,   95,  198, ...,    0,    0,    0],
        [8214, 4479, 7990, ...,    0,    0,    0],
        ...,
        [8214,  584,   12, ...,    0,    0,    0],
        [8214,   59, 1548, ...,    0,    0,    0],
        [8214,  118,   34, ...,    0,    0,    0]])&gt;,
 &lt;tf.Tensor: shape=(64, 40), dtype=int64, numpy=
 array([[8087,   98,   25, ...,    0,    0,    0],
        [8087,   12,   20, ...,    0,    0,    0],
        [8087,   12, 5453, ...,    0,    0,    0],
        ...,
        [8087,   18, 2059, ...,    0,    0,    0],
        [8087,   16, 1436, ...,    0,    0,    0],
        [8087,   15,   57, ...,    0,    0,    0]])&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="positional-encoding">
<h2>Positional encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading">#</a></h2>
<p>Since this model doesn’t contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.</p>
<p>The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the <em>similarity of their meaning and their position in the sentence</em>, in the d-dimensional space.</p>
<p>See the notebook on <a class="reference external" href="https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb">positional encoding</a> to learn more about it. The formula for calculating the positional encoding is as follows:</p>
<div class="math notranslate nohighlight">
\[\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} \]</div>
<div class="math notranslate nohighlight">
\[\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angle_rates</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                          <span class="n">d_model</span><span class="p">)</span>

  <span class="c1"># apply sin to even indices in the array; 2i</span>
  <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>

  <span class="c1"># apply cos to odd indices in the array; 2i+1</span>
  <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>

  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">angle_rads</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;RdBu&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1, 50, 512)
</pre></div>
</div>
<img alt="../../_images/9a0dd2a21ac25f6e79f0288f441cb21510cf78dd33715dabe62e0a0417168fbf.png" src="../../_images/9a0dd2a21ac25f6e79f0288f441cb21510cf78dd33715dabe62e0a0417168fbf.png" />
</div>
</div>
</section>
<section id="masking">
<h2>Masking<a class="headerlink" href="#masking" title="Link to this heading">#</a></h2>
<p>Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value <code class="docutils literal notranslate"><span class="pre">0</span></code> is present: it outputs a <code class="docutils literal notranslate"><span class="pre">1</span></code> at those locations, and a <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
  <span class="n">seq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="c1"># add extra dimensions to add the padding</span>
  <span class="c1"># to the attention logits.</span>
  <span class="k">return</span> <span class="n">seq</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, 1, seq_len)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="n">create_padding_mask</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=
array([[[[0., 0., 1., 1., 0.]]],


       [[[0., 0., 0., 1., 1.]]],


       [[[1., 1., 1., 0., 0.]]]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
<p>The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.</p>
<p>This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">band_part</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">mask</span>  <span class="c1"># (seq_len, seq_len)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">temp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[0., 1., 1.],
       [0., 0., 1.],
       [0., 0., 0.]], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention">
<h2>Scaled dot product attention<a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading">#</a></h2>
<img src="https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png" width="500" alt="scaled_dot_product_attention">
<p>The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:</p>
<div class="math notranslate nohighlight">
\[\Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k}}) V} \]</div>
<p>The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.</p>
<p>For example, consider that <code class="docutils literal notranslate"><span class="pre">Q</span></code> and <code class="docutils literal notranslate"><span class="pre">K</span></code> have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of <code class="docutils literal notranslate"><span class="pre">dk</span></code>. Hence, <em>square root of <code class="docutils literal notranslate"><span class="pre">dk</span></code></em> is used for scaling (and not any other number) because the matmul of <code class="docutils literal notranslate"><span class="pre">Q</span></code> and <code class="docutils literal notranslate"><span class="pre">K</span></code> should have a mean of 0 and variance of 1, and you get a gentler softmax.</p>
<p>The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Calculate the attention weights.</span>
<span class="sd">  q, k, v must have matching leading dimensions.</span>
<span class="sd">  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.</span>
<span class="sd">  The mask has different shapes depending on its type(padding or look ahead)</span>
<span class="sd">  but it must be broadcastable for addition.</span>

<span class="sd">  Args:</span>
<span class="sd">    q: query shape == (..., seq_len_q, depth)</span>
<span class="sd">    k: key shape == (..., seq_len_k, depth)</span>
<span class="sd">    v: value shape == (..., seq_len_v, depth_v)</span>
<span class="sd">    mask: Float tensor with shape broadcastable</span>
<span class="sd">          to (..., seq_len_q, seq_len_k). Defaults to None.</span>

<span class="sd">  Returns:</span>
<span class="sd">    output, attention_weights</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">matmul_qk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

  <span class="c1"># scale matmul_qk</span>
  <span class="n">dk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">scaled_attention_logits</span> <span class="o">=</span> <span class="n">matmul_qk</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

  <span class="c1"># add the mask to the scaled tensor.</span>
  <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">scaled_attention_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

  <span class="c1"># softmax is normalized on the last axis (seq_len_k) so that the scores</span>
  <span class="c1"># add up to 1.</span>
  <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_attention_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, seq_len_k)</span>

  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (..., seq_len_q, depth_v)</span>

  <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<p>As the softmax normalization is done on K, its values decide the amount of importance given to Q.</p>
<p>The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_out</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
  <span class="n">temp_out</span><span class="p">,</span> <span class="n">temp_attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
      <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Attention weights are:&#39;</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_attn</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Output is:&#39;</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="n">temp_out</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">temp_k</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 3)</span>

<span class="n">temp_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span>   <span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span>  <span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">6</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (4, 2)</span>

<span class="c1"># This `query` aligns with the second `key`,</span>
<span class="c1"># so the second `value` is returned.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention weights are:
tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This query aligns with a repeated key (third and fourth),</span>
<span class="c1"># so all associated values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention weights are:
tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This query aligns equally with the first and second key,</span>
<span class="c1"># so their values get averaged.</span>
<span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (1, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention weights are:
tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)
Output is:
tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Pass all the queries together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">temp_q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># (3, 3)</span>
<span class="n">print_out</span><span class="p">(</span><span class="n">temp_q</span><span class="p">,</span> <span class="n">temp_k</span><span class="p">,</span> <span class="n">temp_v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention weights are:
tf.Tensor(
[[0.  0.  0.5 0.5]
 [0.  1.  0.  0. ]
 [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)
Output is:
tf.Tensor(
[[550.    5.5]
 [ 10.    0. ]
 [  5.5   0. ]], shape=(3, 2), dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="multi-head-attention">
<h2>Multi-head attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h2>
<img src="https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png" width="500" alt="multi-head attention">
<p>Multi-head attention consists of four parts:</p>
<ul class="simple">
<li><p>Linear layers and split into heads.</p></li>
<li><p>Scaled dot-product attention.</p></li>
<li><p>Concatenation of heads.</p></li>
<li><p>Final linear layer.</p></li>
</ul>
<p>Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using <code class="docutils literal notranslate"><span class="pre">tf.transpose</span></code>, and <code class="docutils literal notranslate"><span class="pre">tf.reshape</span></code>) and put through a final <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer.</p>
<p>Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>

    <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split the last dimension into (num_heads, depth).</span>
<span class="sd">    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">q</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>

    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_k, depth)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>  <span class="c1"># (batch_size, num_heads, seq_len_v, depth)</span>

    <span class="c1"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="c1"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    <span class="n">scaled_attention</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

    <span class="n">scaled_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># (batch_size, seq_len_q, num_heads, depth)</span>

    <span class="n">concat_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scaled_attention</span><span class="p">,</span>
                                  <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>  <span class="c1"># (batch_size, seq_len_q, d_model)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">concat_attention</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len_q, d_model)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<p>Create a <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> layer to try out. At each location in the sequence, <code class="docutils literal notranslate"><span class="pre">y</span></code>, the <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">temp_mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>  <span class="c1"># (batch_size, encoder_sequence, d_model)</span>
<span class="n">out</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">temp_mha</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="point-wise-feed-forward-network">
<h2>Point wise feed forward network<a class="headerlink" href="#point-wise-feed-forward-network" title="Link to this heading">#</a></h2>
<p>Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>  <span class="c1"># (batch_size, seq_len, dff)</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, d_model)</span>
  <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">sample_ffn</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([64, 50, 512])
</pre></div>
</div>
</div>
</div>
</section>
<section id="encoder-and-decoder">
<h2>Encoder and decoder<a class="headerlink" href="#encoder-and-decoder" title="Link to this heading">#</a></h2>
<img src="https://www.tensorflow.org/images/tutorials/transformer/transformer.png" width="600" alt="transformer"><p>The transformer model follows the same general pattern as a standard <a class="reference internal" href="#nmt_with_attention.ipynb"><span class="xref myst">sequence to sequence with attention model</span></a>.</p>
<ul class="simple">
<li><p>The input sentence is passed through <code class="docutils literal notranslate"><span class="pre">N</span></code> encoder layers that generates an output for each word/token in the sequence.</p></li>
<li><p>The decoder attends on the encoder’s output and its own input (self-attention) to predict the next word.</p></li>
</ul>
<section id="encoder-layer">
<h3>Encoder layer<a class="headerlink" href="#encoder-layer" title="Link to this heading">#</a></h3>
<p>Each encoder layer consists of sublayers:</p>
<ol class="arabic simple">
<li><p>Multi-head attention (with padding mask)</p></li>
<li><p>Point wise feed forward networks.</p></li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.</p>
<p>The output of each sublayer is <code class="docutils literal notranslate"><span class="pre">LayerNorm(x</span> <span class="pre">+</span> <span class="pre">Sublayer(x))</span></code>. The normalization is done on the <code class="docutils literal notranslate"><span class="pre">d_model</span></code> (last) axis. There are N encoder layers in the transformer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>

    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">out1</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>

    <span class="k">return</span> <span class="n">out2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_encoder_layer</span> <span class="o">=</span> <span class="n">EncoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span> <span class="o">=</span> <span class="n">sample_encoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_encoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([64, 43, 512])
</pre></div>
</div>
</div>
</div>
</section>
<section id="decoder-layer">
<h3>Decoder layer<a class="headerlink" href="#decoder-layer" title="Link to this heading">#</a></h3>
<p>Each decoder layer consists of sublayers:</p>
<ol class="arabic simple">
<li><p>Masked multi-head attention (with look ahead mask and padding mask)</p></li>
<li><p>Multi-head attention (with padding mask). V (value) and K (key) receive the <em>encoder output</em> as inputs. Q (query) receives the <em>output from the masked multi-head attention sublayer.</em></p></li>
<li><p>Point wise feed forward networks</p></li>
</ol>
<p>Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is <code class="docutils literal notranslate"><span class="pre">LayerNorm(x</span> <span class="pre">+</span> <span class="pre">Sublayer(x))</span></code>. The normalization is done on the <code class="docutils literal notranslate"><span class="pre">d_model</span></code> (last) axis.</p>
<p>There are N decoder layers in the transformer.</p>
<p>As Q receives the output from decoder’s first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder’s input based on the encoder’s output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">point_wise_feed_forward_network</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dff</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>


  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>
    <span class="c1"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>

    <span class="n">attn1</span><span class="p">,</span> <span class="n">attn_weights_block1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn1</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">attn1</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">attn2</span><span class="p">,</span> <span class="n">attn_weights_block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha2</span><span class="p">(</span>
        <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">attn2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">attn2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn2</span> <span class="o">+</span> <span class="n">out1</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>

    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>
    <span class="n">out3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm3</span><span class="p">(</span><span class="n">ffn_output</span> <span class="o">+</span> <span class="n">out2</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>

    <span class="k">return</span> <span class="n">out3</span><span class="p">,</span> <span class="n">attn_weights_block1</span><span class="p">,</span> <span class="n">attn_weights_block2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_decoder_layer</span> <span class="o">=</span> <span class="n">DecoderLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_decoder_layer</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">sample_encoder_layer_output</span><span class="p">,</span>
    <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">sample_decoder_layer_output</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([64, 50, 512])
</pre></div>
</div>
</div>
</div>
</section>
<section id="encoder">
<h3>Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> consists of:</p>
<ol class="arabic simple">
<li><p>Input Embedding</p></li>
<li><p>Positional Encoding</p></li>
<li><p>N encoder layers</p></li>
</ol>
<p>The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span>
               <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>


    <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># adding embedding and position encoding.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span>
                         <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">62</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">sample_encoder_output</span> <span class="o">=</span> <span class="n">sample_encoder</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">sample_encoder_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (batch_size, input_seq_len, d_model)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(64, 62, 512)
</pre></div>
</div>
</div>
</div>
</section>
<section id="decoder">
<h3>Decoder<a class="headerlink" href="#decoder" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Decoder</span></code> consists of:</p>
<ol class="arabic simple">
<li><p>Output Embedding</p></li>
<li><p>Positional Encoding</p></li>
<li><p>N decoder layers</p></li>
</ol>
<p>The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span>
               <span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">maximum_position_encoding</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>
                       <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">):</span>

    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, target_seq_len, d_model)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dec_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span>
                                             <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="p">)</span>

      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">&#39;decoder_layer</span><span class="si">{}</span><span class="s1">_block1&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block1</span>
      <span class="n">attention_weights</span><span class="p">[</span><span class="s1">&#39;decoder_layer</span><span class="si">{}</span><span class="s1">_block2&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">block2</span>

    <span class="c1"># x.shape == (batch_size, target_seq_len, d_model)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                         <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span>
                         <span class="n">maximum_position_encoding</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">sample_decoder</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span>
                              <span class="n">enc_output</span><span class="o">=</span><span class="n">sample_encoder_output</span><span class="p">,</span>
                              <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attn</span><span class="p">[</span><span class="s1">&#39;decoder_layer2_block2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="create-the-transformer">
<h2>Create the Transformer<a class="headerlink" href="#create-the-transformer" title="Link to this heading">#</a></h2>
<p>Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span> <span class="n">input_vocab_size</span><span class="p">,</span>
               <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">pe_input</span><span class="p">,</span> <span class="n">pe_target</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                           <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">pe_input</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                           <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">pe_target</span><span class="p">,</span> <span class="n">rate</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">,</span>
           <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">):</span>

    <span class="n">enc_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">enc_padding_mask</span><span class="p">)</span>  <span class="c1"># (batch_size, inp_seq_len, d_model)</span>

    <span class="c1"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
    <span class="n">dec_output</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
        <span class="n">tar</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span><span class="p">)</span>

    <span class="n">final_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>

    <span class="k">return</span> <span class="n">final_output</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">input_vocab_size</span><span class="o">=</span><span class="mi">8500</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span>
    <span class="n">pe_input</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">pe_target</span><span class="o">=</span><span class="mi">6000</span><span class="p">)</span>

<span class="n">temp_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">38</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">temp_target</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">36</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">fn_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_transformer</span><span class="p">(</span><span class="n">temp_input</span><span class="p">,</span> <span class="n">temp_target</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">enc_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">look_ahead_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">dec_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">fn_out</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># (batch_size, tar_seq_len, target_vocab_size)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorShape([64, 36, 8000])
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-hyperparameters">
<h2>Set hyperparameters<a class="headerlink" href="#set-hyperparameters" title="Link to this heading">#</a></h2>
<p>To keep this example small and relatively fast, the values for <em>num_layers, d_model, and dff</em> have been reduced.</p>
<p>The values used in the base model of transformer were; <em>num_layers=6</em>, <em>d_model = 512</em>, <em>dff = 2048</em>. See the <a class="reference external" href="https://arxiv.org/abs/1706.03762">paper</a> for all the other versions of the transformer.</p>
<p>Note: By changing the values below, you can get the model that achieved state of the art on many tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">dff</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">input_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">target_vocab_size</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimizer">
<h2>Optimizer<a class="headerlink" href="#optimizer" title="Link to this heading">#</a></h2>
<p>Use the Adam optimizer with a custom learning rate scheduler according to the formula in the <a class="reference external" href="https://arxiv.org/abs/1706.03762">paper</a>.</p>
<div class="math notranslate nohighlight">
\[\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CustomSchedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CustomSchedule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="n">arg1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="n">arg2</span> <span class="o">=</span> <span class="n">step</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
                                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">temp_learning_rate_schedule</span> <span class="o">=</span> <span class="n">CustomSchedule</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">temp_learning_rate_schedule</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Train Step&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;Train Step&#39;)
</pre></div>
</div>
<img alt="../../_images/a22f4bbaf94f530fc09e6ec3923d3d4832d2e17288082c5e8eec111bb0887c30.png" src="../../_images/a22f4bbaf94f530fc09e6ec3923d3d4832d2e17288082c5e8eec111bb0887c30.png" />
</div>
</div>
</section>
<section id="loss-and-metrics">
<h2>Loss and metrics<a class="headerlink" href="#loss-and-metrics" title="Link to this heading">#</a></h2>
<p>Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_object</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
  <span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_object</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>

  <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">loss_</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">loss_</span> <span class="o">*=</span> <span class="n">mask</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss_</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_loss&#39;</span><span class="p">)</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-and-checkpointing">
<h2>Training and checkpointing<a class="headerlink" href="#training-and-checkpointing" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dff</span><span class="p">,</span>
                          <span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span>
                          <span class="n">pe_input</span><span class="o">=</span><span class="n">input_vocab_size</span><span class="p">,</span>
                          <span class="n">pe_target</span><span class="o">=</span><span class="n">target_vocab_size</span><span class="p">,</span>
                          <span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="c1"># Encoder padding mask</span>
  <span class="n">enc_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

  <span class="c1"># Used in the 2nd attention block in the decoder.</span>
  <span class="c1"># This padding mask is used to mask the encoder outputs.</span>
  <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

  <span class="c1"># Used in the 1st attention block in the decoder.</span>
  <span class="c1"># It is used to pad and mask future tokens in the input received by</span>
  <span class="c1"># the decoder.</span>
  <span class="n">look_ahead_mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tar</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">dec_target_padding_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">tar</span><span class="p">)</span>
  <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dec_target_padding_mask</span><span class="p">,</span> <span class="n">look_ahead_mask</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span>
</pre></div>
</div>
</div>
</div>
<p>Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every <code class="docutils literal notranslate"><span class="pre">n</span></code> epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;./checkpoints/train&quot;</span>

<span class="n">ckpt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Checkpoint</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">ckpt_manager</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointManager</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="k">if</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">:</span>
  <span class="n">ckpt</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">ckpt_manager</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">)</span>
  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Latest checkpoint restored!!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. <code class="docutils literal notranslate"><span class="pre">tar_real</span></code> is that same input shifted by 1: At each location in <code class="docutils literal notranslate"><span class="pre">tar_input</span></code>, <code class="docutils literal notranslate"><span class="pre">tar_real</span></code> contains the  next token that should be predicted.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">sentence</span></code> = “SOS A lion in the jungle is sleeping EOS”</p>
<p><code class="docutils literal notranslate"><span class="pre">tar_inp</span></code> =  “SOS A lion in the jungle is sleeping”</p>
<p><code class="docutils literal notranslate"><span class="pre">tar_real</span></code> = “A lion in the jungle is sleeping EOS”</p>
<p>The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.</p>
<p>During training this example uses teacher-forcing (like in the <a class="reference internal" href="#./text_generation.ipynb"><span class="xref myst">text generation tutorial</span></a>). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.</p>
<p>As the transformer predicts each word, <em>self-attention</em> allows it to look at the previous words in the input sequence to better predict the next word.</p>
<p>To prevent the model from peeking at the expected output the model uses a look-ahead mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The @tf.function trace-compiles train_step into a TF graph for faster</span>
<span class="c1"># execution. The function specializes to the precise shape of the argument</span>
<span class="c1"># tensors. To avoid re-tracing due to the variable sequence lengths or variable</span>
<span class="c1"># batch sizes (the last batch is smaller), use input_signature to specify</span>
<span class="c1"># more generic shapes.</span>

<span class="n">train_step_signature</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
<span class="p">]</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="n">train_step_signature</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">):</span>
  <span class="n">tar_inp</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">tar_real</span> <span class="o">=</span> <span class="n">tar</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

  <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar_inp</span><span class="p">,</span>
                                 <span class="kc">True</span><span class="p">,</span>
                                 <span class="n">enc_padding_mask</span><span class="p">,</span>
                                 <span class="n">combined_mask</span><span class="p">,</span>
                                 <span class="n">dec_padding_mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

  <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

  <span class="n">train_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
  <span class="n">train_accuracy</span><span class="p">(</span><span class="n">tar_real</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Portuguese is used as the input language and English is the target language.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
  <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

  <span class="n">train_loss</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>
  <span class="n">train_accuracy</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>

  <span class="c1"># inp -&gt; portuguese, tar -&gt; english</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">):</span>
    <span class="n">train_step</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">tar</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1"> Batch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span> <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ckpt_save_path</span> <span class="o">=</span> <span class="n">ckpt_manager</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Saving checkpoint for epoch </span><span class="si">{}</span><span class="s1"> at </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
                                                         <span class="n">ckpt_save_path</span><span class="p">))</span>

  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1"> Loss </span><span class="si">{:.4f}</span><span class="s1"> Accuracy </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                                <span class="n">train_loss</span><span class="o">.</span><span class="n">result</span><span class="p">(),</span>
                                                <span class="n">train_accuracy</span><span class="o">.</span><span class="n">result</span><span class="p">()))</span>

  <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Time taken for 1 epoch: </span><span class="si">{}</span><span class="s1"> secs</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1 Batch 0 Loss 8.9950 Accuracy 0.0004
Epoch 1 Batch 50 Loss 8.9483 Accuracy 0.0020
Epoch 1 Batch 100 Loss 8.8548 Accuracy 0.0129
Epoch 1 Batch 150 Loss 8.7534 Accuracy 0.0175
Epoch 1 Batch 200 Loss 8.6268 Accuracy 0.0199
Epoch 1 Batch 250 Loss 8.4743 Accuracy 0.0213
Epoch 1 Batch 300 Loss 8.3000 Accuracy 0.0222
Epoch 1 Batch 350 Loss 8.1148 Accuracy 0.0252
Epoch 1 Batch 400 Loss 7.9316 Accuracy 0.0285
Epoch 1 Batch 450 Loss 7.7665 Accuracy 0.0317
Epoch 1 Batch 500 Loss 7.6198 Accuracy 0.0353
Epoch 1 Batch 550 Loss 7.4844 Accuracy 0.0394
Epoch 1 Batch 600 Loss 7.3570 Accuracy 0.0435
Epoch 1 Batch 650 Loss 7.2362 Accuracy 0.0473
Epoch 1 Batch 700 Loss 7.1195 Accuracy 0.0511
Epoch 1 Loss 7.1148 Accuracy 0.0512
Time taken for 1 epoch: 81.85551714897156 secs

Epoch 2 Batch 0 Loss 5.5560 Accuracy 0.1062
Epoch 2 Batch 50 Loss 5.4624 Accuracy 0.1048
Epoch 2 Batch 100 Loss 5.4135 Accuracy 0.1055
Epoch 2 Batch 150 Loss 5.3616 Accuracy 0.1074
Epoch 2 Batch 200 Loss 5.3105 Accuracy 0.1094
Epoch 2 Batch 250 Loss 5.2679 Accuracy 0.1114
Epoch 2 Batch 300 Loss 5.2335 Accuracy 0.1133
Epoch 2 Batch 350 Loss 5.1986 Accuracy 0.1149
Epoch 2 Batch 400 Loss 5.1633 Accuracy 0.1165
Epoch 2 Batch 450 Loss 5.1301 Accuracy 0.1179
Epoch 2 Batch 500 Loss 5.1015 Accuracy 0.1192
Epoch 2 Batch 550 Loss 5.0753 Accuracy 0.1204
Epoch 2 Batch 600 Loss 5.0524 Accuracy 0.1216
Epoch 2 Batch 650 Loss 5.0296 Accuracy 0.1229
Epoch 2 Batch 700 Loss 5.0049 Accuracy 0.1242
Epoch 2 Loss 5.0040 Accuracy 0.1242
Time taken for 1 epoch: 51.96726894378662 secs

Epoch 3 Batch 0 Loss 4.6851 Accuracy 0.1504
Epoch 3 Batch 50 Loss 4.6035 Accuracy 0.1428
Epoch 3 Batch 100 Loss 4.6047 Accuracy 0.1438
Epoch 3 Batch 150 Loss 4.5796 Accuracy 0.1441
Epoch 3 Batch 200 Loss 4.5817 Accuracy 0.1444
Epoch 3 Batch 250 Loss 4.5698 Accuracy 0.1448
Epoch 3 Batch 300 Loss 4.5566 Accuracy 0.1455
Epoch 3 Batch 350 Loss 4.5409 Accuracy 0.1459
Epoch 3 Batch 400 Loss 4.5316 Accuracy 0.1463
Epoch 3 Batch 450 Loss 4.5203 Accuracy 0.1473
Epoch 3 Batch 500 Loss 4.5073 Accuracy 0.1479
Epoch 3 Batch 550 Loss 4.4937 Accuracy 0.1484
Epoch 3 Batch 600 Loss 4.4795 Accuracy 0.1494
Epoch 3 Batch 650 Loss 4.4629 Accuracy 0.1501
Epoch 3 Batch 700 Loss 4.4484 Accuracy 0.1510
Epoch 3 Loss 4.4477 Accuracy 0.1510
Time taken for 1 epoch: 52.078697204589844 secs

Epoch 4 Batch 0 Loss 4.3831 Accuracy 0.1705
Epoch 4 Batch 50 Loss 4.1242 Accuracy 0.1661
Epoch 4 Batch 100 Loss 4.1239 Accuracy 0.1670
Epoch 4 Batch 150 Loss 4.1054 Accuracy 0.1678
Epoch 4 Batch 200 Loss 4.0842 Accuracy 0.1692
Epoch 4 Batch 250 Loss 4.0768 Accuracy 0.1703
Epoch 4 Batch 300 Loss 4.0666 Accuracy 0.1709
Epoch 4 Batch 350 Loss 4.0512 Accuracy 0.1723
Epoch 4 Batch 400 Loss 4.0329 Accuracy 0.1735
Epoch 4 Batch 450 Loss 4.0170 Accuracy 0.1744
Epoch 4 Batch 500 Loss 4.0016 Accuracy 0.1758
Epoch 4 Batch 550 Loss 3.9853 Accuracy 0.1770
Epoch 4 Batch 600 Loss 3.9679 Accuracy 0.1780
Epoch 4 Batch 650 Loss 3.9537 Accuracy 0.1790
Epoch 4 Batch 700 Loss 3.9399 Accuracy 0.1798
Epoch 4 Loss 3.9394 Accuracy 0.1798
Time taken for 1 epoch: 52.19272303581238 secs

Epoch 5 Batch 0 Loss 3.6505 Accuracy 0.2063
Epoch 5 Batch 50 Loss 3.5814 Accuracy 0.1988
Epoch 5 Batch 100 Loss 3.5736 Accuracy 0.1994
Epoch 5 Batch 150 Loss 3.5764 Accuracy 0.1990
Epoch 5 Batch 200 Loss 3.5738 Accuracy 0.2002
Epoch 5 Batch 250 Loss 3.5629 Accuracy 0.2007
Epoch 5 Batch 300 Loss 3.5570 Accuracy 0.2022
Epoch 5 Batch 350 Loss 3.5445 Accuracy 0.2029
Epoch 5 Batch 400 Loss 3.5322 Accuracy 0.2037
Epoch 5 Batch 450 Loss 3.5217 Accuracy 0.2040
Epoch 5 Batch 500 Loss 3.5089 Accuracy 0.2047
Epoch 5 Batch 550 Loss 3.5023 Accuracy 0.2052
Epoch 5 Batch 600 Loss 3.4944 Accuracy 0.2058
Epoch 5 Batch 650 Loss 3.4879 Accuracy 0.2061
Epoch 5 Batch 700 Loss 3.4774 Accuracy 0.2067
Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1
Epoch 5 Loss 3.4777 Accuracy 0.2066
Time taken for 1 epoch: 52.16017556190491 secs

Epoch 6 Batch 0 Loss 3.0016 Accuracy 0.2085
Epoch 6 Batch 50 Loss 3.1487 Accuracy 0.2205
Epoch 6 Batch 100 Loss 3.1540 Accuracy 0.2222
Epoch 6 Batch 150 Loss 3.1469 Accuracy 0.2229
Epoch 6 Batch 200 Loss 3.1413 Accuracy 0.2231
Epoch 6 Batch 250 Loss 3.1409 Accuracy 0.2236
Epoch 6 Batch 300 Loss 3.1389 Accuracy 0.2240
Epoch 6 Batch 350 Loss 3.1328 Accuracy 0.2243
Epoch 6 Batch 400 Loss 3.1243 Accuracy 0.2252
Epoch 6 Batch 450 Loss 3.1155 Accuracy 0.2253
Epoch 6 Batch 500 Loss 3.1103 Accuracy 0.2258
Epoch 6 Batch 550 Loss 3.1041 Accuracy 0.2261
Epoch 6 Batch 600 Loss 3.0977 Accuracy 0.2265
Epoch 6 Batch 650 Loss 3.0907 Accuracy 0.2271
Epoch 6 Batch 700 Loss 3.0814 Accuracy 0.2275
Epoch 6 Loss 3.0810 Accuracy 0.2276
Time taken for 1 epoch: 51.977559089660645 secs

Epoch 7 Batch 0 Loss 2.7907 Accuracy 0.2260
Epoch 7 Batch 50 Loss 2.7461 Accuracy 0.2409
Epoch 7 Batch 100 Loss 2.7480 Accuracy 0.2422
Epoch 7 Batch 150 Loss 2.7444 Accuracy 0.2431
Epoch 7 Batch 200 Loss 2.7400 Accuracy 0.2440
Epoch 7 Batch 250 Loss 2.7395 Accuracy 0.2443
Epoch 7 Batch 300 Loss 2.7342 Accuracy 0.2444
Epoch 7 Batch 350 Loss 2.7285 Accuracy 0.2451
Epoch 7 Batch 400 Loss 2.7253 Accuracy 0.2461
Epoch 7 Batch 450 Loss 2.7171 Accuracy 0.2466
Epoch 7 Batch 500 Loss 2.7087 Accuracy 0.2472
Epoch 7 Batch 550 Loss 2.6997 Accuracy 0.2474
Epoch 7 Batch 600 Loss 2.6941 Accuracy 0.2483
Epoch 7 Batch 650 Loss 2.6915 Accuracy 0.2486
Epoch 7 Batch 700 Loss 2.6884 Accuracy 0.2489
Epoch 7 Loss 2.6876 Accuracy 0.2489
Time taken for 1 epoch: 51.867215156555176 secs

Epoch 8 Batch 0 Loss 2.1733 Accuracy 0.3074
Epoch 8 Batch 50 Loss 2.3754 Accuracy 0.2606
Epoch 8 Batch 100 Loss 2.3772 Accuracy 0.2639
Epoch 8 Batch 150 Loss 2.3865 Accuracy 0.2647
Epoch 8 Batch 200 Loss 2.3877 Accuracy 0.2649
Epoch 8 Batch 250 Loss 2.3827 Accuracy 0.2649
Epoch 8 Batch 300 Loss 2.3819 Accuracy 0.2650
Epoch 8 Batch 350 Loss 2.3767 Accuracy 0.2652
Epoch 8 Batch 400 Loss 2.3745 Accuracy 0.2655
Epoch 8 Batch 450 Loss 2.3742 Accuracy 0.2656
Epoch 8 Batch 500 Loss 2.3740 Accuracy 0.2660
Epoch 8 Batch 550 Loss 2.3730 Accuracy 0.2666
Epoch 8 Batch 600 Loss 2.3691 Accuracy 0.2667
Epoch 8 Batch 650 Loss 2.3707 Accuracy 0.2664
Epoch 8 Batch 700 Loss 2.3691 Accuracy 0.2668
Epoch 8 Loss 2.3689 Accuracy 0.2668
Time taken for 1 epoch: 51.68695878982544 secs

Epoch 9 Batch 0 Loss 1.8818 Accuracy 0.3027
Epoch 9 Batch 50 Loss 2.0871 Accuracy 0.2792
Epoch 9 Batch 100 Loss 2.1128 Accuracy 0.2795
Epoch 9 Batch 150 Loss 2.1130 Accuracy 0.2805
Epoch 9 Batch 200 Loss 2.1212 Accuracy 0.2807
Epoch 9 Batch 250 Loss 2.1283 Accuracy 0.2797
Epoch 9 Batch 300 Loss 2.1315 Accuracy 0.2793
Epoch 9 Batch 350 Loss 2.1286 Accuracy 0.2795
Epoch 9 Batch 400 Loss 2.1317 Accuracy 0.2800
Epoch 9 Batch 450 Loss 2.1328 Accuracy 0.2799
Epoch 9 Batch 500 Loss 2.1330 Accuracy 0.2802
Epoch 9 Batch 550 Loss 2.1353 Accuracy 0.2799
Epoch 9 Batch 600 Loss 2.1364 Accuracy 0.2798
Epoch 9 Batch 650 Loss 2.1385 Accuracy 0.2797
Epoch 9 Batch 700 Loss 2.1391 Accuracy 0.2797
Epoch 9 Loss 2.1396 Accuracy 0.2797
Time taken for 1 epoch: 51.850889444351196 secs

Epoch 10 Batch 0 Loss 2.0022 Accuracy 0.2720
Epoch 10 Batch 50 Loss 1.9100 Accuracy 0.2937
Epoch 10 Batch 100 Loss 1.9124 Accuracy 0.2942
Epoch 10 Batch 150 Loss 1.9255 Accuracy 0.2918
Epoch 10 Batch 200 Loss 1.9296 Accuracy 0.2915
Epoch 10 Batch 250 Loss 1.9339 Accuracy 0.2913
Epoch 10 Batch 300 Loss 1.9378 Accuracy 0.2918
Epoch 10 Batch 350 Loss 1.9416 Accuracy 0.2920
Epoch 10 Batch 400 Loss 1.9468 Accuracy 0.2921
Epoch 10 Batch 450 Loss 1.9488 Accuracy 0.2924
Epoch 10 Batch 500 Loss 1.9517 Accuracy 0.2920
Epoch 10 Batch 550 Loss 1.9533 Accuracy 0.2916
Epoch 10 Batch 600 Loss 1.9568 Accuracy 0.2912
Epoch 10 Batch 650 Loss 1.9592 Accuracy 0.2909
Epoch 10 Batch 700 Loss 1.9632 Accuracy 0.2907
Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2
Epoch 10 Loss 1.9629 Accuracy 0.2908
Time taken for 1 epoch: 52.364691972732544 secs

Epoch 11 Batch 0 Loss 1.9845 Accuracy 0.3025
Epoch 11 Batch 50 Loss 1.7652 Accuracy 0.3041
Epoch 11 Batch 100 Loss 1.7750 Accuracy 0.3031
Epoch 11 Batch 150 Loss 1.7855 Accuracy 0.3006
Epoch 11 Batch 200 Loss 1.7929 Accuracy 0.3001
Epoch 11 Batch 250 Loss 1.7966 Accuracy 0.3001
Epoch 11 Batch 300 Loss 1.7996 Accuracy 0.3002
Epoch 11 Batch 350 Loss 1.8024 Accuracy 0.2997
Epoch 11 Batch 400 Loss 1.8035 Accuracy 0.2995
Epoch 11 Batch 450 Loss 1.8067 Accuracy 0.2999
Epoch 11 Batch 500 Loss 1.8109 Accuracy 0.2998
Epoch 11 Batch 550 Loss 1.8131 Accuracy 0.2994
Epoch 11 Batch 600 Loss 1.8169 Accuracy 0.2995
Epoch 11 Batch 650 Loss 1.8207 Accuracy 0.2992
Epoch 11 Batch 700 Loss 1.8230 Accuracy 0.2992
Epoch 11 Loss 1.8230 Accuracy 0.2992
Time taken for 1 epoch: 51.926491260528564 secs

Epoch 12 Batch 0 Loss 1.6864 Accuracy 0.3502
Epoch 12 Batch 50 Loss 1.6227 Accuracy 0.3104
Epoch 12 Batch 100 Loss 1.6334 Accuracy 0.3107
Epoch 12 Batch 150 Loss 1.6469 Accuracy 0.3101
Epoch 12 Batch 200 Loss 1.6600 Accuracy 0.3098
Epoch 12 Batch 250 Loss 1.6666 Accuracy 0.3094
Epoch 12 Batch 300 Loss 1.6681 Accuracy 0.3086
Epoch 12 Batch 350 Loss 1.6756 Accuracy 0.3083
Epoch 12 Batch 400 Loss 1.6803 Accuracy 0.3086
Epoch 12 Batch 450 Loss 1.6857 Accuracy 0.3089
Epoch 12 Batch 500 Loss 1.6911 Accuracy 0.3085
Epoch 12 Batch 550 Loss 1.6943 Accuracy 0.3081
Epoch 12 Batch 600 Loss 1.6978 Accuracy 0.3080
Epoch 12 Batch 650 Loss 1.7019 Accuracy 0.3075
Epoch 12 Batch 700 Loss 1.7046 Accuracy 0.3073
Epoch 12 Loss 1.7048 Accuracy 0.3073
Time taken for 1 epoch: 52.1059148311615 secs

Epoch 13 Batch 0 Loss 1.4554 Accuracy 0.3493
Epoch 13 Batch 50 Loss 1.5281 Accuracy 0.3164
Epoch 13 Batch 100 Loss 1.5345 Accuracy 0.3183
Epoch 13 Batch 150 Loss 1.5494 Accuracy 0.3171
Epoch 13 Batch 200 Loss 1.5535 Accuracy 0.3168
Epoch 13 Batch 250 Loss 1.5606 Accuracy 0.3161
Epoch 13 Batch 300 Loss 1.5646 Accuracy 0.3161
Epoch 13 Batch 350 Loss 1.5703 Accuracy 0.3158
Epoch 13 Batch 400 Loss 1.5759 Accuracy 0.3151
Epoch 13 Batch 450 Loss 1.5795 Accuracy 0.3147
Epoch 13 Batch 500 Loss 1.5848 Accuracy 0.3147
Epoch 13 Batch 550 Loss 1.5917 Accuracy 0.3140
Epoch 13 Batch 600 Loss 1.5945 Accuracy 0.3138
Epoch 13 Batch 650 Loss 1.6011 Accuracy 0.3135
Epoch 13 Batch 700 Loss 1.6055 Accuracy 0.3137
Epoch 13 Loss 1.6059 Accuracy 0.3138
Time taken for 1 epoch: 52.238577127456665 secs

Epoch 14 Batch 0 Loss 1.4919 Accuracy 0.3295
Epoch 14 Batch 50 Loss 1.4296 Accuracy 0.3264
Epoch 14 Batch 100 Loss 1.4389 Accuracy 0.3221
Epoch 14 Batch 150 Loss 1.4527 Accuracy 0.3223
Epoch 14 Batch 200 Loss 1.4636 Accuracy 0.3214
Epoch 14 Batch 250 Loss 1.4750 Accuracy 0.3203
Epoch 14 Batch 300 Loss 1.4856 Accuracy 0.3199
Epoch 14 Batch 350 Loss 1.4865 Accuracy 0.3202
Epoch 14 Batch 400 Loss 1.4901 Accuracy 0.3209
Epoch 14 Batch 450 Loss 1.4958 Accuracy 0.3206
Epoch 14 Batch 500 Loss 1.5015 Accuracy 0.3200
Epoch 14 Batch 550 Loss 1.5087 Accuracy 0.3196
Epoch 14 Batch 600 Loss 1.5112 Accuracy 0.3196
Epoch 14 Batch 650 Loss 1.5155 Accuracy 0.3191
Epoch 14 Batch 700 Loss 1.5190 Accuracy 0.3190
Epoch 14 Loss 1.5193 Accuracy 0.3190
Time taken for 1 epoch: 52.14116668701172 secs

Epoch 15 Batch 0 Loss 1.5438 Accuracy 0.3092
Epoch 15 Batch 50 Loss 1.3760 Accuracy 0.3292
Epoch 15 Batch 100 Loss 1.3777 Accuracy 0.3253
Epoch 15 Batch 150 Loss 1.3828 Accuracy 0.3266
Epoch 15 Batch 200 Loss 1.3888 Accuracy 0.3282
Epoch 15 Batch 250 Loss 1.3966 Accuracy 0.3275
Epoch 15 Batch 300 Loss 1.4039 Accuracy 0.3274
Epoch 15 Batch 350 Loss 1.4076 Accuracy 0.3277
Epoch 15 Batch 400 Loss 1.4099 Accuracy 0.3271
Epoch 15 Batch 450 Loss 1.4140 Accuracy 0.3269
Epoch 15 Batch 500 Loss 1.4214 Accuracy 0.3260
Epoch 15 Batch 550 Loss 1.4273 Accuracy 0.3252
Epoch 15 Batch 600 Loss 1.4335 Accuracy 0.3250
Epoch 15 Batch 650 Loss 1.4393 Accuracy 0.3247
Epoch 15 Batch 700 Loss 1.4445 Accuracy 0.3246
Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3
Epoch 15 Loss 1.4445 Accuracy 0.3246
Time taken for 1 epoch: 52.58523750305176 secs

Epoch 16 Batch 0 Loss 1.1730 Accuracy 0.3433
Epoch 16 Batch 50 Loss 1.2793 Accuracy 0.3313
Epoch 16 Batch 100 Loss 1.2946 Accuracy 0.3314
Epoch 16 Batch 150 Loss 1.3047 Accuracy 0.3301
Epoch 16 Batch 200 Loss 1.3103 Accuracy 0.3298
Epoch 16 Batch 250 Loss 1.3213 Accuracy 0.3298
Epoch 16 Batch 300 Loss 1.3289 Accuracy 0.3300
Epoch 16 Batch 350 Loss 1.3361 Accuracy 0.3301
Epoch 16 Batch 400 Loss 1.3441 Accuracy 0.3297
Epoch 16 Batch 450 Loss 1.3501 Accuracy 0.3297
Epoch 16 Batch 500 Loss 1.3567 Accuracy 0.3295
Epoch 16 Batch 550 Loss 1.3636 Accuracy 0.3287
Epoch 16 Batch 600 Loss 1.3689 Accuracy 0.3286
Epoch 16 Batch 650 Loss 1.3736 Accuracy 0.3289
Epoch 16 Batch 700 Loss 1.3774 Accuracy 0.3287
Epoch 16 Loss 1.3778 Accuracy 0.3287
Time taken for 1 epoch: 52.2056667804718 secs

Epoch 17 Batch 0 Loss 1.2018 Accuracy 0.3438
Epoch 17 Batch 50 Loss 1.2523 Accuracy 0.3374
Epoch 17 Batch 100 Loss 1.2415 Accuracy 0.3376
Epoch 17 Batch 150 Loss 1.2514 Accuracy 0.3359
Epoch 17 Batch 200 Loss 1.2589 Accuracy 0.3371
Epoch 17 Batch 250 Loss 1.2624 Accuracy 0.3361
Epoch 17 Batch 300 Loss 1.2700 Accuracy 0.3350
Epoch 17 Batch 350 Loss 1.2774 Accuracy 0.3353
Epoch 17 Batch 400 Loss 1.2857 Accuracy 0.3344
Epoch 17 Batch 450 Loss 1.2906 Accuracy 0.3346
Epoch 17 Batch 500 Loss 1.2951 Accuracy 0.3340
Epoch 17 Batch 550 Loss 1.3001 Accuracy 0.3340
Epoch 17 Batch 600 Loss 1.3052 Accuracy 0.3339
Epoch 17 Batch 650 Loss 1.3115 Accuracy 0.3337
Epoch 17 Batch 700 Loss 1.3174 Accuracy 0.3333
Epoch 17 Loss 1.3172 Accuracy 0.3332
Time taken for 1 epoch: 52.01518774032593 secs

Epoch 18 Batch 0 Loss 1.2578 Accuracy 0.3510
Epoch 18 Batch 50 Loss 1.1775 Accuracy 0.3444
Epoch 18 Batch 100 Loss 1.1908 Accuracy 0.3414
Epoch 18 Batch 150 Loss 1.2021 Accuracy 0.3408
Epoch 18 Batch 200 Loss 1.2074 Accuracy 0.3409
Epoch 18 Batch 250 Loss 1.2141 Accuracy 0.3409
Epoch 18 Batch 300 Loss 1.2167 Accuracy 0.3409
Epoch 18 Batch 350 Loss 1.2233 Accuracy 0.3412
Epoch 18 Batch 400 Loss 1.2269 Accuracy 0.3412
Epoch 18 Batch 450 Loss 1.2352 Accuracy 0.3405
Epoch 18 Batch 500 Loss 1.2418 Accuracy 0.3398
Epoch 18 Batch 550 Loss 1.2476 Accuracy 0.3395
Epoch 18 Batch 600 Loss 1.2522 Accuracy 0.3389
Epoch 18 Batch 650 Loss 1.2568 Accuracy 0.3385
Epoch 18 Batch 700 Loss 1.2624 Accuracy 0.3378
Epoch 18 Loss 1.2628 Accuracy 0.3379
Time taken for 1 epoch: 52.02039384841919 secs

Epoch 19 Batch 0 Loss 0.9466 Accuracy 0.3572
Epoch 19 Batch 50 Loss 1.1196 Accuracy 0.3412
Epoch 19 Batch 100 Loss 1.1403 Accuracy 0.3451
Epoch 19 Batch 150 Loss 1.1554 Accuracy 0.3432
Epoch 19 Batch 200 Loss 1.1582 Accuracy 0.3433
Epoch 19 Batch 250 Loss 1.1650 Accuracy 0.3431
Epoch 19 Batch 300 Loss 1.1714 Accuracy 0.3428
Epoch 19 Batch 350 Loss 1.1812 Accuracy 0.3421
Epoch 19 Batch 400 Loss 1.1842 Accuracy 0.3416
Epoch 19 Batch 450 Loss 1.1868 Accuracy 0.3420
Epoch 19 Batch 500 Loss 1.1936 Accuracy 0.3418
Epoch 19 Batch 550 Loss 1.1985 Accuracy 0.3418
Epoch 19 Batch 600 Loss 1.2038 Accuracy 0.3421
Epoch 19 Batch 650 Loss 1.2101 Accuracy 0.3415
Epoch 19 Batch 700 Loss 1.2148 Accuracy 0.3406
Epoch 19 Loss 1.2148 Accuracy 0.3406
Time taken for 1 epoch: 51.859150886535645 secs

Epoch 20 Batch 0 Loss 1.0080 Accuracy 0.3522
Epoch 20 Batch 50 Loss 1.0766 Accuracy 0.3502
Epoch 20 Batch 100 Loss 1.0950 Accuracy 0.3493
Epoch 20 Batch 150 Loss 1.1066 Accuracy 0.3498
Epoch 20 Batch 200 Loss 1.1162 Accuracy 0.3485
Epoch 20 Batch 250 Loss 1.1202 Accuracy 0.3479
Epoch 20 Batch 300 Loss 1.1262 Accuracy 0.3474
Epoch 20 Batch 350 Loss 1.1307 Accuracy 0.3472
Epoch 20 Batch 400 Loss 1.1362 Accuracy 0.3468
Epoch 20 Batch 450 Loss 1.1435 Accuracy 0.3464
Epoch 20 Batch 500 Loss 1.1507 Accuracy 0.3451
Epoch 20 Batch 550 Loss 1.1556 Accuracy 0.3445
Epoch 20 Batch 600 Loss 1.1597 Accuracy 0.3445
Epoch 20 Batch 650 Loss 1.1653 Accuracy 0.3442
Epoch 20 Batch 700 Loss 1.1696 Accuracy 0.3438
Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4
Epoch 20 Loss 1.1698 Accuracy 0.3437
Time taken for 1 epoch: 52.15591549873352 secs
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate">
<h2>Evaluate<a class="headerlink" href="#evaluate" title="Link to this heading">#</a></h2>
<p>The following steps are used for evaluation:</p>
<ul class="simple">
<li><p>Encode the input sentence using the Portuguese tokenizer (<code class="docutils literal notranslate"><span class="pre">tokenizer_pt</span></code>). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.</p></li>
<li><p>The decoder input is the <code class="docutils literal notranslate"><span class="pre">start</span> <span class="pre">token</span> <span class="pre">==</span> <span class="pre">tokenizer_en.vocab_size</span></code>.</p></li>
<li><p>Calculate the padding masks and the look ahead masks.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">decoder</span></code> then outputs the predictions by looking at the <code class="docutils literal notranslate"><span class="pre">encoder</span> <span class="pre">output</span></code> and its own output (self-attention).</p></li>
<li><p>Select the last word and calculate the argmax of that.</p></li>
<li><p>Concatentate the predicted word to the decoder input as pass it to the decoder.</p></li>
<li><p>In this approach, the decoder predicts the next word based on the previous words it predicted.</p></li>
</ul>
<p>Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">):</span>
  <span class="n">start_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">end_token</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

  <span class="c1"># inp sentence is portuguese, hence adding the start and end token</span>
  <span class="n">inp_sentence</span> <span class="o">=</span> <span class="n">start_token</span> <span class="o">+</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">)</span> <span class="o">+</span> <span class="n">end_token</span>
  <span class="n">encoder_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inp_sentence</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="c1"># as the target is english, the first word to the transformer should be the</span>
  <span class="c1"># english start token.</span>
  <span class="n">decoder_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="n">enc_padding_mask</span><span class="p">,</span> <span class="n">combined_mask</span><span class="p">,</span> <span class="n">dec_padding_mask</span> <span class="o">=</span> <span class="n">create_masks</span><span class="p">(</span>
        <span class="n">encoder_input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="c1"># predictions.shape == (batch_size, seq_len, vocab_size)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="n">encoder_input</span><span class="p">,</span>
                                                 <span class="n">output</span><span class="p">,</span>
                                                 <span class="kc">False</span><span class="p">,</span>
                                                 <span class="n">enc_padding_mask</span><span class="p">,</span>
                                                 <span class="n">combined_mask</span><span class="p">,</span>
                                                 <span class="n">dec_padding_mask</span><span class="p">)</span>

    <span class="c1"># select the last word from the seq_len dimension</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># (batch_size, 1, vocab_size)</span>

    <span class="n">predicted_id</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># return the result if the predicted_id is equal to the end token</span>
    <span class="k">if</span> <span class="n">predicted_id</span> <span class="o">==</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>

    <span class="c1"># concatentate the predicted_id to the output which is given to the decoder</span>
    <span class="c1"># as its input.</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">predicted_id</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_attention_weights</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

  <span class="n">sentence</span> <span class="o">=</span> <span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

  <span class="n">attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># plot the attention weights</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">attention</span><span class="p">[</span><span class="n">head</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>

    <span class="n">fontdict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fontsize&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)))</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span>
        <span class="p">[</span><span class="s1">&#39;&lt;start&gt;&#39;</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="n">tokenizer_pt</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;&lt;end&gt;&#39;</span><span class="p">],</span>
        <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span>
                        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">],</span>
                       <span class="n">fontdict</span><span class="o">=</span><span class="n">fontdict</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Head </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
  <span class="n">result</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

  <span class="n">predicted_sentence</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result</span>
                                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">])</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Predicted translation: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_sentence</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
    <span class="n">plot_attention_weights</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">plot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;este é um problema que temos que resolver.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: this is a problem we have to solve .&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input: este é um problema que temos que resolver.
Predicted translation: this is a problem we have to solve it .
Real translation: this is a problem we have to solve .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;os meus vizinhos ouviram sobre esta ideia.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: and my neighboring homes heard about this idea .&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input: os meus vizinhos ouviram sobre esta ideia.
Predicted translation: my neighbors heard about this idea .
Real translation: and my neighboring homes heard about this idea .
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: so i &#39;ll just share with you some stories very quickly of some magical things that have happened .&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.
Predicted translation: so i &#39;m going to very quickly to share with you some of the magic stories that have happened .
Real translation: so i &#39;ll just share with you some stories very quickly of some magical things that have happened .
</pre></div>
</div>
</div>
</div>
<p>You can pass different layers and attention blocks of the decoder to the <code class="docutils literal notranslate"><span class="pre">plot</span></code> parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;este é o primeiro livro que eu fiz.&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="s1">&#39;decoder_layer4_block2&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Real translation: this is the first book i&#39;ve ever done.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input: este é o primeiro livro que eu fiz.
Predicted translation: this is the first book i made .
</pre></div>
</div>
<img alt="../../_images/3c7b13d96ef0ae4702ac39849efc5a70e16b96ac00c1be6b6711f5830077c251.png" src="../../_images/3c7b13d96ef0ae4702ac39849efc5a70e16b96ac00c1be6b6711f5830077c251.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Real translation: this is the first book i&#39;ve ever done.
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.</p>
<p>Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT</a> and train state of the art models. Futhermore, you can implement beam search to get better predictions.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters\chpt7"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-input-pipeline">Setup input pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masking">Masking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled dot product attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-wise-feed-forward-network">Point wise feed forward network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-and-decoder">Encoder and decoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-layer">Encoder layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-layer">Decoder layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-transformer">Create the Transformer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-hyperparameters">Set hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer">Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-and-metrics">Loss and metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-checkpointing">Training and checkpointing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate">Evaluate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://fangli-ying.github.io/">Dr. Fangli Ying</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>