{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMEyRqRZdbmr"
   },
   "source": [
    "# 3. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XmPU1nWHO5w"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://fangli-ying.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWbSYUc4dg21"
   },
   "source": [
    "在第二章中，我们探索了用于模型训练的数据并检查了数据特征。在第3章中，我们将检查如何预处理时间序列数据。\n",
    "\n",
    "为了将时间序列数据转换为监督学习问题，必须将数据处理成一对要预测的目标变量和用于预测的输入变量。此外，为了稳定地训练深度学习模型，需要统一数据的规模。在第 3.1 节中，我们将研究将确诊的冠状病毒数据转换为用于监督学习的数据的过程，在第 3.2 节中，我们将检查如何扩展数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LATMT-vOiBai"
   },
   "source": [
    "## 3.1 构建监督学习数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emYeU_6yvDfc"
   },
   "source": [
    "对于数据预处理实践，我们将使用 2.1 节中所示的代码加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5589,
     "status": "ok",
     "timestamp": 1608432119294,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "jmgiP7hDihW_",
    "outputId": "b5c826cf-9fc9-486f-8f7b-944bfd03830d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'D:\\3000-code\\deeplearning\\DeepLearning2023\\Deeplearning\\chapters\\chpt4\\Tutorial-Book-Utils\\PL_data_loader.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
    "!python Tutorial-Book-Utils/PL_data_loader.py --data COVIDTimeSeries\n",
    "!unzip -q COVIDTimeSeries.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypeC4OifvaK3"
   },
   "source": [
    "使用2.3节中的代码，daily_cases我们将计算韩国每天的确诊病例数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 1478,
     "status": "ok",
     "timestamp": 1608432122003,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "LDrsndsCihdp",
    "outputId": "7329ccdc-bbc0-4c47-ef73-661889b5a02c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'time_series_covid19_confirmed_global.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m confirmed \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_series_covid19_confirmed_global.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m confirmed[confirmed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry/Region\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKorea, South\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m korea \u001b[38;5;241m=\u001b[39m confirmed[confirmed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry/Region\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKorea, South\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'time_series_covid19_confirmed_global.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "confirmed = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
    "confirmed[confirmed['Country/Region']=='Korea, South']\n",
    "korea = confirmed[confirmed['Country/Region']=='Korea, South'].iloc[:,4:].T\n",
    "korea.index = pd.to_datetime(korea.index)\n",
    "daily_cases = korea.diff().fillna(korea.iloc[0]).astype('int')\n",
    "daily_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRzJXo3Cx4qQ"
   },
   "source": [
    "为了使上述时间序列数据能够被模型用于监督学习，必须将数据处理成输入变量和目标变量对。在时间序列问题中，此类数据也称为序列数据。为了将其处理成序列数据，必须首先定义序列长度。序列长度决定了有多少过去的数据可以预测未来。例如，如果序列长度为5t\n",
    "过去来预测某个时间点 $t-1$, $t-2$, $t-3$, $t-4$, $t-5$ 您将利用该时间点的数据。像这样 $t-k$ 从터 $t-1$ 数据高达 $t$ 预测时间称为一步预测任务。\n",
    "下面定义的函数create_sequences将大小为N的时间序列数据转换为N-seq_length的监督学习数据，如图3-1所示。\n",
    "\n",
    "![](https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/TS-ch3img01.png?raw=true)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY9hvmnaBbay"
   },
   "source": [
    "- 图3-1 时间序列数据转换流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04UXWETidbOD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x = data.iloc[i:(i+seq_length)]\n",
    "        y = data.iloc[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 5\n",
    "X, y = create_sequences(daily_cases, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0-zPOcd8p6p"
   },
   "source": [
    "seq_length通过定义 5 并将create_sequences函数应用daily_cases到 ，可以看到总共构建了 327 个监督学习数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1784,
     "status": "ok",
     "timestamp": 1608432122365,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "4vQ_PEk0OVTN",
    "outputId": "da775f36-0206-4f07-b777-9d2a195fea24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327, 5, 1), (327, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4erMbiGaBBep"
   },
   "source": [
    "我们将构建的数据分为学习数据、验证数据和测试数据。我们将以 8:1:1 的比例分离数据。 327 的 80% 大约为 261，因此我们将使用前 261 个数据进行训练，接下来的 33 个数据进行验证，最后 33 个数据进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1766,
     "status": "ok",
     "timestamp": 1608432122366,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "JqJVumBC8409",
    "outputId": "1c952458-d720-4253-a5eb-9bd3c8da7925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "train_size = int(327 * 0.8)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9HpHIR-Oh2T"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+33], y[train_size:train_size+33]\n",
    "X_test, y_test = X[train_size+33:], y[train_size+33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1608432122368,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "8EeX5aOoOa0l",
    "outputId": "904a97c3-5256-4468-ad28-5a67ce708294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 5, 1) (33, 5, 1) (33, 5, 1)\n",
      "(261, 1) (33, 1) (33, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ05Fk12jke3"
   },
   "source": [
    "## 3.2 数据缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdLKCYi3CI1i"
   },
   "source": [
    "在本节中，我们将执行数据缩放。我们将执行 MinMax 缩放，将数据范围转换为 0 到 1 之间。 MinMax 缩放计算数据集的最小值和最大值，并使用以下公式应用它们\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYhCqFRbCuUd"
   },
   "source": [
    ">$x_{scaled} = \\displaystyle\\frac{x - x_{min}}{x_{max} - x_{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGA3sPDvCv1A"
   },
   "source": [
    "扩展时要记住的一件事是，您必须使用训练数据的统计数据来扩展训练、验证和测试数据集。由于在学习模型时不应输入测试数据的信息，因此使用训练数据的统计量对训练数据进行缩放。由于模型是使用根据训练数据的统计数据缩放的数据来学习的，因此用于未来模型性能评估的测试数据输入也根据训练数据的统计数据缩放。同样，验证数据必须经过与测试数据相同的预处理过程，因此它会根据训练数据的统计数据进行缩放。\n",
    "\n",
    "要应用 MinMax 缩放，X_train我们将找到数据的最小值和最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1729,
     "status": "ok",
     "timestamp": 1608432122369,
     "user": {
      "displayName": "안성진",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
      "userId": "00266029492778998652"
     },
     "user_tz": -540
    },
    "id": "Qmh0K8N4Glvb",
    "outputId": "421775e8-0a4d-4ff3-8357-527fe5c0c080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 851\n"
     ]
    }
   ],
   "source": [
    "MIN = X_train.min()\n",
    "MAX = X_train.max()\n",
    "print(MIN, MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrD05cTgHgIS"
   },
   "source": [
    "最小值为 0，最大值为 851。接下来，我们将定义 MinMax 缩放函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGdcy31yHrla"
   },
   "outputs": [],
   "source": [
    "def MinMaxScale(array, min, max):\n",
    "\n",
    "    return (array - min) / (max - min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzYEOKK7Hw_R"
   },
   "source": [
    "MinMaxScale我们将使用函数进行扩展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhJo_3FQH2HE"
   },
   "outputs": [],
   "source": [
    "X_train = MinMaxScale(X_train, MIN, MAX)\n",
    "y_train = MinMaxScale(y_train, MIN, MAX)\n",
    "X_val = MinMaxScale(X_val, MIN, MAX)\n",
    "y_val = MinMaxScale(y_val, MIN, MAX)\n",
    "X_test = MinMaxScale(X_test, MIN, MAX)\n",
    "y_test = MinMaxScale(y_test, MIN, MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhDGDK-cIJOd"
   },
   "source": [
    "接下来，我们将np.array数据类型转换为要输入到 PyTorch 模型中的类型。torch.Tensor首先，我们定义一个转换数据类型的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1bRuXBDIIdZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_Tensor(array):\n",
    "    return torch.from_numpy(array).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMRYCpW6IeM_"
   },
   "source": [
    "make_Tensor我们将使用函数来转换数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxXjlAetInNg"
   },
   "outputs": [],
   "source": [
    "X_train = make_Tensor(X_train)\n",
    "y_train = make_Tensor(y_train)\n",
    "X_val = make_Tensor(X_val)\n",
    "y_val = make_Tensor(y_val)\n",
    "X_test = make_Tensor(X_test)\n",
    "y_test = make_Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07vuEYiQJ08-"
   },
   "source": [
    "到目前为止，我们已经了解了如何将时间序列数据转换为用于监督学习的数据以及如何缩放数据。在下一章中，我们将使用构建的数据构建冠状病毒确诊病例的预测模型。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch3. 데이터 전처리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}