
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>第一章 深度学习基础 &#8212; 深度学习</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/chpt1/DeepLearning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1. 目标检测简介" href="../chpt2/Ch1-Object-Detection.html" />
    <link rel="prev" title="欢迎来到《深度学习原理及其应用》" href="../../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/dp.gif" class="logo__image only-light" alt="深度学习 - Home"/>
    <img src="../../_static/dp.gif" class="logo__image only-dark pst-js-only" alt="深度学习 - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    欢迎来到《深度学习原理及其应用》
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">深度学习基础</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">第一章 深度学习基础</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">视觉基本任务</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch1-Object-Detection.html">1. 目标检测简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch2-EDA.html">2. 数据探索</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch4-RetinaNet.html">4. RetinaNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch5-Faster-R-CNN.html">5. Faster R-CNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt2/Ch5-References.html">6. References</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">深度生成模型</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch1-Introduction.html">1. 生成对抗网络（GAN）简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch2-EDA.html">2. EDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch3-GAN.html">3. GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch4-pix2pix.html">4. pix2pix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt3/Ch5-VAE.html">Variational Autoencoder（未完成）</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">序列模型</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch1-Time-Series.html">1. Time Series 时间序列简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch2-EDA.html">2. EDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch3-preprocessing.html">3. 数据预处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch4-LSTM.html">4. LSTM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chpt4/Ch5-CNN-LSTM.html">5. CNN-LSTM</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">自然语言处理</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt5/Ch1-Introduction.html">1. NLP模型简介 (Introduction to NLP Model)（未完待续）</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">图表示学习</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chpt6/graph.html">第六章 图理论基础 （未完待续）</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/Fangli-Ying/DeepLearning2023/master?urlpath=tree/chapters/chpt1/DeepLearning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://jupyter.org/hub/hub/user-redirect/git-pull?repo=https%3A//github.com/Fangli-Ying/DeepLearning2023&urlpath=tree/DeepLearning2023/chapters/chpt1/DeepLearning.ipynb&branch=master" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/Fangli-Ying/DeepLearning2023/blob/master/chapters/chpt1/DeepLearning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/chpt1/DeepLearning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>第一章 深度学习基础</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">第一章 深度学习基础</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1 神经网络及其基本组成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.1 神经网络的结构</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">第一章 深度学习基础</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">3.1 神经网络及其基本组成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">3.1.1 神经网络的结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.1.2 输出层与损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.1.3 模型优化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.1.4 模型的过拟合与欠拟合</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.1.5 前向传播和反向传播</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.1.6 神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3.2 卷积神经网络</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.2.1 卷积</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.2.2 池化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">3.2.3 卷积神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.3 循环神经网络</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">3.3.1 循环神经网络架构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">3.3.2 循环神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">3.4 参考引用</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>第一章 深度学习基础<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>深度学习通过构建和训练多层神经网络来学习数据表示和模式。深度学习在各个领域都取得了巨大的成功，包括计算机视觉、自然语言处理、语音识别和推荐系统等。深度学习的核心是神经网络模型，它由许多神经元组成，这些神经元按层次组织在一起。输入数据从网络的输入层传递到输出层，期间经过多个中间层，这些层被称为隐藏层。每个神经元都执行一些计算，并将结果传递给下一层的神经元，以此实现复杂的数据处理和特征提取。深度学习的一个关键优势是它可以从原始数据中学习特征表示，而无需手动设计特征提取器。这种自动学习特征表示的能力使得深度学习在许多任务上表现出色。</p>
<section id="id2">
<h2>1.1 神经网络及其基本组成<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>1.1.1 神经网络的结构<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>最常见的朴素神经网络一般指<strong>多层感知机</strong>。多层感知机包含输入层，隐藏层和输出层，它们由多层神经元组成， 每一层与它的上一层相连。</p>
<p><img alt="logit" src="../../_images/3_neutral-network-diagram.png" /></p>
<p>当去掉多层感知机的隐藏层后，它就是最初始的单层感知机模型。这些感知机模型，因为没有反馈回路，只有前馈路径，也被叫做前馈网络。而前馈网络除了感知机模型外还有许多其他的网络，但是它们在深度学习时代并不是那么流行和通用。</p>
<p>假设我们有一个输入矩阵 <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span>，其中 <span class="math notranslate nohighlight">\(d\)</span> 是输入的维度，<span class="math notranslate nohighlight">\(n\)</span> 是输入样本的数量。单层感知机就是对输入矩阵，通过权重向量 <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{d \times h}\)</span> 和 偏置项 <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{1 \times h}\)</span> 进行线性加权，然后通过一个激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span>，得到输出 <span class="math notranslate nohighlight">\(\hat{Y} \in \mathbb{R}^{n \times q}\)</span>，写作</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} = \sigma(WX + B).
\]</div>
<p>我们将每一层权重 <span class="math notranslate nohighlight">\(W\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, 和激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span> 统一写作 <span class="math notranslate nohighlight">\(f^{(l)}\)</span>，那么当我们增加到 <span class="math notranslate nohighlight">\(L\)</span> 个隐藏层后得到的多层感知机模型可以写为</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} = f^{(L)}(\dots f^{(l)}(\dots f^{(2)}(f^{(1)}(X)))).
\]</div>
<p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。这里的激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span> 可以是</p>
<ul class="simple">
<li><p>ReLU 函数： <span class="math notranslate nohighlight">\(ReLU(x) = max(x, 0)\)</span></p></li>
<li><p>Sigmoid 函数：<span class="math notranslate nohighlight">\(sigmoid(x) = \frac{1}{1 + exp(-x)}\)</span></p></li>
<li><p>Tanh 函数：<span class="math notranslate nohighlight">\(tanh(x) = \frac{1 - exp(-2x)}{1 + exp(-2x)}\)</span>
<img alt="logit" src="../../_images/3_activation_functions.png" /></p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id4">
<h1>第一章 深度学习基础<a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<p>深度学习通过构建和训练多层神经网络来学习数据表示和模式。深度学习在各个领域都取得了巨大的成功，包括计算机视觉、自然语言处理、语音识别和推荐系统等。深度学习的核心是神经网络模型，它由许多神经元组成，这些神经元按层次组织在一起。输入数据从网络的输入层传递到输出层，期间经过多个中间层，这些层被称为隐藏层。每个神经元都执行一些计算，并将结果传递给下一层的神经元，以此实现复杂的数据处理和特征提取。深度学习的一个关键优势是它可以从原始数据中学习特征表示，而无需手动设计特征提取器。这种自动学习特征表示的能力使得深度学习在许多任务上表现出色。</p>
<section id="id5">
<h2>3.1 神经网络及其基本组成<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>3.1.1 神经网络的结构<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>最常见的朴素神经网络一般指<strong>多层感知机</strong>。多层感知机包含输入层，隐藏层和输出层，它们由多层神经元组成， 每一层与它的上一层相连。
<img alt="logit" src="../../_images/3_neutral-network-diagram.png" /></p>
<p>当去掉多层感知机的隐藏层后，它就是最初始的单层感知机模型。这些感知机模型，因为没有反馈回路，只有前馈路径，也被叫做前馈网络。而前馈网络除了感知机模型外还有许多其他的网络，但是它们在深度学习时代并不是那么流行和通用。</p>
<p>假设我们有一个输入矩阵 <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times d}\)</span>，其中 <span class="math notranslate nohighlight">\(d\)</span> 是输入的维度，<span class="math notranslate nohighlight">\(n\)</span> 是输入样本的数量。单层感知机就是对输入矩阵，通过权重向量 <span class="math notranslate nohighlight">\(W \in \mathbb{R}^{d \times h}\)</span> 和 偏置项 <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{1 \times h}\)</span> 进行线性加权，然后通过一个激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span>，得到输出 <span class="math notranslate nohighlight">\(\hat{Y} \in \mathbb{R}^{n \times q}\)</span>，写作</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} = \sigma(WX + B).
\]</div>
<p>我们将每一层权重 <span class="math notranslate nohighlight">\(W\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, 和激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span> 统一写作 <span class="math notranslate nohighlight">\(f^{(l)}\)</span>，那么当我们增加到 <span class="math notranslate nohighlight">\(L\)</span> 个隐藏层后得到的多层感知机模型可以写为</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} = f^{(L)}(\dots f^{(l)}(\dots f^{(2)}(f^{(1)}(X)))).
\]</div>
<p>激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。这里的激活函数 <span class="math notranslate nohighlight">\(\sigma\)</span> 可以是</p>
<ul class="simple">
<li><p>ReLU 函数： <span class="math notranslate nohighlight">\(ReLU(x) = max(x, 0)\)</span></p></li>
<li><p>Sigmoid 函数：<span class="math notranslate nohighlight">\(sigmoid(x) = \frac{1}{1 + exp(-x)}\)</span></p></li>
<li><p>Tanh 函数：<span class="math notranslate nohighlight">\(tanh(x) = \frac{1 - exp(-2x)}{1 + exp(-2x)}\)</span>
<img alt="logit" src="../../_images/3_activation_functions.png" /></p></li>
</ul>
</section>
<section id="id7">
<h3>3.1.2 输出层与损失函数<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>假设神经网络的前一层网络的输出为 <span class="math notranslate nohighlight">\(h\)</span>，输出层（最后一层） <span class="math notranslate nohighlight">\(L\)</span> 的表达式为
$<span class="math notranslate nohighlight">\(
\hat{y} = \sigma(W^{(L)}h + B^{(L)}).
\)</span>$</p>
<p><strong>回归问题</strong></p>
<p>对于一般的回归问题，我们可以认为 <span class="math notranslate nohighlight">\(\sigma\)</span> 是一个恒等映射，那么</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \sigma(W^{(L)}h + B^{(L)}) = W^{(L)}h + B^{(L)}.
\]</div>
<p>然后，我们可以计算 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 和 <span class="math notranslate nohighlight">\(y\)</span> 的平方差作为损失函数：</p>
<div class="math notranslate nohighlight">
\[
l(y, \hat{y}) = (y - \hat{y})^2.
\]</div>
<p><strong>二分类问题</strong></p>
<p>对于二分类问题，我们假定其输出是 <span class="math notranslate nohighlight">\(0\)</span> 或者 <span class="math notranslate nohighlight">\(1\)</span>。因此，我们可以使用 sigmoid 函数将输出的范围控制在 0 到 1 之间，即</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = sigmoid(W^{(L)}h + B^{(L)})
\]</div>
<p>然后，我们可以使用交叉熵作为损失函数计算让预测值 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 和 真实值 <span class="math notranslate nohighlight">\(y\)</span> 更接近：</p>
<div class="math notranslate nohighlight">
\[
l(y, \hat{y}) = -y log(\hat{y}) - (1-y)log(1-\hat{y}).
\]</div>
<p>在测试的时候，我们可以使用阈值函数将输出 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 转换为二元类别标签。例如，当 <span class="math notranslate nohighlight">\(\hat{y} \geq 0.5\)</span> 时，我们预测为正类别，否则预测为负类别。</p>
<p><strong>多分类问题</strong></p>
<p>对于多分类或者 <span class="math notranslate nohighlight">\(k\)</span> 分类问题，其输出可以被定义为一个 one-hot 向量 <span class="math notranslate nohighlight">\(y \in \{0, 1\}^k\)</span>，其中第 <span class="math notranslate nohighlight">\(j\)</span> 个元素 <span class="math notranslate nohighlight">\(y_j=1\)</span> 表示这个样本的标签为 <span class="math notranslate nohighlight">\(j\)</span> （标签从 0 开始）。因此，为了将输出目标转化为一个类似的分布，我们需要使用 softmax 函数作为函数对输出进行标准化，即 <span class="math notranslate nohighlight">\(\sigma(\cdot) = softmax(\cdot)\)</span>得到输出向量的概率分布</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = softmax(W^{(L)}h + B^{(L)}),
\]</div>
<p>其中，softmax 函数被定义为</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_j = softmax(z)_j=\frac{exp(z_j)}{\sum_k exp(z_k)}.
\]</div>
<p>对于多分类问题，我们依然可以使用交叉熵作为损失函数</p>
<div class="math notranslate nohighlight">
\[
l(y,\hat{y})= -\sum_{j=1}^{n}y_jlog\hat{y}_j.
\]</div>
</section>
<section id="id8">
<h3>3.1.3 模型优化<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>截止到目前，我们已经使用了许多优化算法来训练深度学习模型。优化算法使我们能够继续更新模型参数，并使损失函数的值最小化。下面我们将更加详细介绍两个最常用的优化算法：</p>
<ul class="simple">
<li><p>随机梯度下降算法</p></li>
<li><p>Adam 算法</p></li>
</ul>
<p>此外，还有许多优秀的优化算法，比如动量法、AdaGrad 算法、RMSProp 算法、Adadelta 算法等等。
<img alt="logit" src="../../_images/3_gd.PNG" /></p>
<p><strong>随机梯度下降算法</strong></p>
<p>在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定 <span class="math notranslate nohighlight">\(n\)</span> 个样本的训练数据集，我们假设 <span class="math notranslate nohighlight">\(l_i(\mathbf{x})\)</span> 是关于索引 <span class="math notranslate nohighlight">\(i\)</span> 的训练样本的损失函数，其中 <span class="math notranslate nohighlight">\(x\)</span> 是参数向量。然后我们得到目标函数
$<span class="math notranslate nohighlight">\(
f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\mathbf{x}).
\)</span><span class="math notranslate nohighlight">\(
其梯度计算为
\)</span><span class="math notranslate nohighlight">\(
\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).
\)</span><span class="math notranslate nohighlight">\(
如果使用梯度下降法，则每个自变量迭代的计算代价为 \)</span>\mathcal{O}(n)<span class="math notranslate nohighlight">\(，它随 \)</span>n$ 线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。</p>
<p>随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引 <span class="math notranslate nohighlight">\(i\)</span>，其中 <span class="math notranslate nohighlight">\(i\in\{1,\ldots, n\}\)</span>，并计算梯度 <span class="math notranslate nohighlight">\(\nabla f_i(\mathbf{x})\)</span> 以更新 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:
$<span class="math notranslate nohighlight">\(
\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),
\)</span><span class="math notranslate nohighlight">\(
其中 \)</span>\eta$ 是学习率。</p>
<p><strong>Adam 算法</strong></p>
<p>Adam（Adaptive Moment Estimation）是一种自适应学习率的优化算法，结合了动量法和 RMSProp 算法的优点。</p>
<p>Adam算法的关键组成部分之一是：它使用指数加权移动平均值来估算梯度的动量和二次矩，即它使用状态变量
$<span class="math notranslate nohighlight">\(
\mathbf{v_t} \leftarrow \beta_1 \mathbf{v_{t-1}} + (1 - \beta_1) \mathbf{g_t},
\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(
\mathbf{s_t} \leftarrow \beta_2 \mathbf{s_{t-1}} + (1 - \beta_2) \mathbf{g_t^2}.
\)</span>$</p>
<p>这里 <span class="math notranslate nohighlight">\(\beta_1\)</span> 和 <span class="math notranslate nohighlight">\(\beta_2\)</span> 是非负加权参数。常将它们设置为 <span class="math notranslate nohighlight">\(\beta_1 = 0.9\)</span> 和 <span class="math notranslate nohighlight">\(\beta_2 = 0.999\)</span>。也就是说，方差估计的移动远远慢于动量估计的移动。 如果我们初始化 <span class="math notranslate nohighlight">\(\mathbf{v_0} = \mathbf{s_0} = 0\)</span>，就会获得一个相当大的初始偏差。 我们可以通过使用 <span class="math notranslate nohighlight">\(\sum_{i=0}^t \beta^i = \frac{1 - \beta^t}{1 - \beta}\)</span> 来解决这个问题。相应地，标准化状态变量由下式获得
$<span class="math notranslate nohighlight">\(
\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_1^t} \text{ and } \hat{\mathbf{s}}_t = \frac{\mathbf{s}_t}{1 - \beta_2^t}.
\)</span>$</p>
<p>有了正确的估计，我们现在可以写出更新方程。 首先，我们重新缩放梯度以获得
$<span class="math notranslate nohighlight">\(
\mathbf{g}_t' = \frac{\eta \hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}.
\)</span><span class="math notranslate nohighlight">\(
通常，我们选择 \)</span>\epsilon = 10^{-6}$，这是为了在数值稳定性和逼真度之间取得良好的平衡。</p>
<p>最后，我们更新权重：
$<span class="math notranslate nohighlight">\(
\mathbf{x_t} \leftarrow \mathbf{x_{t-1}} - \mathbf{g_t'}.
\)</span>$</p>
</section>
<section id="id9">
<h3>3.1.4 模型的过拟合与欠拟合<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>当我们比较训练和验证误差时，我们要注意两种常见的情况。</p>
<ul class="simple">
<li><p>欠拟合（underfitting）：训练误差和验证误差都很严重，而且它们之间仅有一点差距。 即，如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足）， 无法捕获试图学习的模式。</p></li>
<li><p>过拟合（overfitting）：当我们的训练误差明显低于验证误差时。注意，过拟合并不总是一件坏事。 特别是在深度学习领域，众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。 最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。</p></li>
</ul>
<p>其中一个影响是否过拟合的原因是数据集的大小。 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。另外一个原因是模型的复杂度，我们将在下面仔细的探讨它，以及它的解决方案。</p>
<p>模型复杂性，如下图所示，</p>
<ul class="simple">
<li><p>当小于最佳的模型复杂度的时候：增加模型复杂度，可以同时降低训练损失和泛化损失。在到达最佳点之前，就是欠拟合的情况。</p></li>
<li><p>当大于最佳的模型复杂度的时候：增加模型复杂度，只能降低训练损失，并且会造成泛化损失的增加。这就是过拟合。
<img alt="logit" src="../../_images/3_model_complexity.png" /></p></li>
</ul>
<p>而针对深度学习而言，训练需要学习的参数量巨大，大多数情况是模型过拟合。以下讨论两种深度学习中常用的缓解过拟合的方法</p>
<ol class="arabic simple">
<li><p>权重衰退</p></li>
<li><p>暂退法</p></li>
</ol>
<p><strong>权重衰退</strong></p>
<p>在训练参数化机器学习模型时，权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为 <span class="math notranslate nohighlight">\(L_2\)</span> 正则化。这项技术通过函数与 <span class="math notranslate nohighlight">\(0\)</span> 的距离来衡量函数的复杂度， 因为在所有函数中，<span class="math notranslate nohighlight">\(f=0\)</span> （所有输入都得到值 <span class="math notranslate nohighlight">\(0\)</span>）在某种意义上是最简单的。</p>
<p>一种简单的方法是通过线性函数 <span class="math notranslate nohighlight">\(f(x)=w^{T}x\)</span> 中的权重向量的某个范数来度量其复杂性， 例如 <span class="math notranslate nohighlight">\(\| w \|^2\)</span>。 要保证权重向量比较小，最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标最小化训练标签上的预测损失，调整为最小化预测损失和惩罚项之和。现在，如果我们的权重向量增长的太大，我们的学习算法可能会更集中于最小化权重范数 <span class="math notranslate nohighlight">\(\| w \|^2\)</span>。假设我们的损失函数由线性回归给出:
$<span class="math notranslate nohighlight">\(L(w,b)= \frac{1}{n}\sum\frac{1}{2}(w^{T}+b-y^{(i)})^2\)</span>$</p>
<p>其中 <span class="math notranslate nohighlight">\(x^{(i)}\)</span> 是样本 <span class="math notranslate nohighlight">\(i\)</span> 的特征，<span class="math notranslate nohighlight">\(y^{(i)}\)</span> 是样本 <span class="math notranslate nohighlight">\(i\)</span> 的标签，<span class="math notranslate nohighlight">\((w,b)\)</span> 是权重和偏置参数。 为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加 <span class="math notranslate nohighlight">\(\| {w} \|^2\)</span>。 但是模型应该如何平衡这个新的额外惩罚的损失？ 实际上，我们通过正则化常数 <span class="math notranslate nohighlight">\(\lambda\)</span> 来描述这种权衡， 这是一个非负超参数。所以，新的损失函数可以定义为：
$<span class="math notranslate nohighlight">\(L(w,b)+\frac{\lambda}{2}\| w \|^2\)</span>$</p>
<p><span class="math notranslate nohighlight">\(L_2\)</span> 正则化线性模型构成经典的岭回归（ridge regression）算法， <span class="math notranslate nohighlight">\(L_1\)</span>正则化线性回归是统计学中类似的基本模型， 通常被称为套索回归（lasso regression）。 使用<span class="math notranslate nohighlight">\(L_2\)</span>范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，<span class="math notranslate nohighlight">\(L_1\)</span>惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。</p>
<p><span class="math notranslate nohighlight">\(L_2\)</span> 正则化回归的小批量随机梯度下降更新公式如下：
$<span class="math notranslate nohighlight">\(
w \leftarrow (1-\eta\lambda)w - \frac{\eta}{|N|}\Sigma_{i\in N} x^{(i)}(w^T x^{(i)}+b-y^{(i)})
\)</span><span class="math notranslate nohighlight">\(
我们根据估计值与观测值之间的差异来更新 \)</span>w<span class="math notranslate nohighlight">\(。 然而，我们同时也在试图将 \)</span>w<span class="math notranslate nohighlight">\( 的大小缩小到零。 这就是为什么这种方法有时被称为**权重衰减**。 我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的 \)</span>w<span class="math notranslate nohighlight">\( 值对应较少约束的\)</span>\lambda<span class="math notranslate nohighlight">\(， 而较大的 \)</span>w$ 值对的约束更大。</p>
<p>是否对相应的偏置 <span class="math notranslate nohighlight">\(b^2\)</span> 进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。</p>
<p><strong>暂退法 Dropout</strong></p>
<p>Dropout 可以被认为是一种集成大量深层神经网络的实用的 Bagging 方法。Bagging 方法的思想是多个模型集成，每个模型都有一部分数据训练，然后所有模型的结果集成起来作为最后的预测结果。Dropout 可以看作是在每个训练批次中训练一个新的神经网络，每个网络只使用一部分神经元（即 Dropout），然后所有网络的结果集成起来作为最后的预测结果。因此，Dropout 方法可以减少模型复杂度，提高泛化能力。</p>
<p>当我们将 Dropout 应用到隐藏层，以 <span class="math notranslate nohighlight">\(p\)</span> 的概率将隐藏单元置为零时，结果可以看作是一个只包含原始神经元子集的网络。比如在下图中，删除了 <span class="math notranslate nohighlight">\(h_2\)</span> 和 <span class="math notranslate nohighlight">\(h_5\)</span>，因此输出的计算不再依赖于 <span class="math notranslate nohighlight">\(h_2\)</span> 或 <span class="math notranslate nohighlight">\(h_5\)</span>，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于 <span class="math notranslate nohighlight">\(h_1,...,h_5\)</span> 的任何一个元素。</p>
<p><img alt="logit" src="../../_images/3_dropout.png" /></p>
</section>
<section id="id10">
<h3>3.1.5 前向传播和反向传播<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>我们已经学习了如何用随机梯度下降训练模型。然而，对于深度学习模型的权重的梯度的计算十分困难，前向传播和反向传播为我们提供了有力的工具。</p>
<p><strong>前向传播</strong></p>
<p>前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果，下面是一张前向传播的计算图,它对应的目标函数是带有权重衰减的损失函数。</p>
<ul class="simple">
<li><p>我们先看图的最下面一行：输入 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>，经过线性变化 <span class="math notranslate nohighlight">\(\mathbf{W}^{(1)}\)</span> 得到 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>；然后，经过非线性变化 <span class="math notranslate nohighlight">\(\phi\)</span>，得到 <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>；然后，经过线性变化 <span class="math notranslate nohighlight">\(\mathbf{W}^{(2)}\)</span>，得到 <span class="math notranslate nohighlight">\(\mathbf{o}\)</span>，最后，通过真实值 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 和预测值 <span class="math notranslate nohighlight">\(\mathbf{o}\)</span> 计算损失得到 <span class="math notranslate nohighlight">\(L\)</span>。</p></li>
<li><p>然后，再看上面的两行计算：我们对权重 <span class="math notranslate nohighlight">\(\mathbf{W}^{(1)}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{W}^{(2)}\)</span> 施加 <span class="math notranslate nohighlight">\(l_2\)</span> 正则化约束得到 <span class="math notranslate nohighlight">\(s\)</span>。最后，将 <span class="math notranslate nohighlight">\(s\)</span> 和 <span class="math notranslate nohighlight">\(L\)</span> 相加得到最终的目标函数 <span class="math notranslate nohighlight">\(J\)</span>。</p></li>
</ul>
<p><img alt="logit" src="../../_images/3_forward.png" /></p>
<p><strong>反向传播</strong></p>
<p>反向传播（backward propagation或backpropagation）的目的是计算网络中需要更新的参数的梯度 <span class="math notranslate nohighlight">\(\partial J/\partial \mathbf{W}^{(1)}\)</span> 和 <span class="math notranslate nohighlight">\(\partial J/\partial \mathbf{W}^{(2)}\)</span>。为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。以 <span class="math notranslate nohighlight">\(\partial J/\partial \mathbf{W}^{(1)}\)</span>  为例，通过链式法则，我们最终可以得到其梯度为
$<span class="math notranslate nohighlight">\(
\frac{\partial J}{\partial \mathbf{W}^{(1)}}
= \frac{\partial J}{\partial \mathbf{z}} \times \frac{\partial \mathbf{z}}{\partial \mathbf{W}^{(1)}} + \frac{\partial J}{\partial s} \times \frac{\partial s}{\partial \mathbf{W}^{(1)}}
= \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.
\)</span>$</p>
</section>
<section id="id11">
<h3>3.1.6 神经网络代码<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>这里我们使用 MNIST 数字识别数据集。因为，对于每一个样本而言，我们的输入必须是一个一维的向量，虽然 MNIST 数据集含有分辨率为 <span class="math notranslate nohighlight">\(28 \times 28\)</span> 的图片，但他们默认被存储成了一个 <span class="math notranslate nohighlight">\(28 \times 28=784\)</span> 的向量。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: torch in d:\program files\python39\lib\site-packages (2.5.1)
Requirement already satisfied: torchvision in d:\program files\python39\lib\site-packages (0.20.1)
Requirement already satisfied: jinja2 in d:\program files\python39\lib\site-packages (from torch) (3.1.4)
Requirement already satisfied: filelock in d:\program files\python39\lib\site-packages (from torch) (3.16.1)
Requirement already satisfied: fsspec in d:\program files\python39\lib\site-packages (from torch) (2024.10.0)
Requirement already satisfied: networkx in d:\program files\python39\lib\site-packages (from torch) (3.2.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in d:\program files\python39\lib\site-packages (from torch) (4.12.2)
Requirement already satisfied: sympy==1.13.1 in d:\program files\python39\lib\site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in d:\program files\python39\lib\site-packages (from sympy==1.13.1-&gt;torch) (1.3.0)
Requirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in d:\program files\python39\lib\site-packages (from torchvision) (9.5.0)
Requirement already satisfied: numpy in d:\program files\python39\lib\site-packages (from torchvision) (1.26.4)
Requirement already satisfied: MarkupSafe&gt;=2.0 in d:\program files\python39\lib\site-packages (from jinja2-&gt;torch) (3.0.2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="c1"># 定义应用于数据的转换</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>

<span class="c1"># 加载 MNIST 数据集</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="c1"># 设置超参数</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># 创建数据加载器</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>接下来，我们定义一个两层的 MLP 模型。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 定义 MLP 模型</span>
<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 初始化模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>然后，我们使用交叉熵函数作为损失函数，使用 Adam 算法作为优化器。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 定义损失函数和优化器</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>最后，我们进行训练与测试</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 训练循环</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># 测试</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy on the test set: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch [1/10], Loss: 0.3384
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>     <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="ne">----&gt; </span><span class="mi">4</span>     <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>         <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>         <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torch\utils\data\dataloader.py:701,</span> in <span class="ni">_BaseDataLoaderIter.__next__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">698</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">699</span>     <span class="c1"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="g g-Whitespace">    </span><span class="mi">700</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>  <span class="c1"># type: ignore[call-arg]</span>
<span class="ne">--&gt; </span><span class="mi">701</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_data</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">702</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span> <span class="k">if</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">704</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span>
<span class="g g-Whitespace">    </span><span class="mi">705</span>     <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">706</span>     <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span>
<span class="g g-Whitespace">    </span><span class="mi">707</span> <span class="p">):</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torch\utils\data\dataloader.py:757,</span> in <span class="ni">_SingleProcessDataLoaderIter._next_data</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">755</span> <span class="k">def</span> <span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">756</span>     <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_index</span><span class="p">()</span>  <span class="c1"># may raise StopIteration</span>
<span class="ne">--&gt; </span><span class="mi">757</span>     <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_fetcher</span><span class="o">.</span><span class="n">fetch</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
<span class="g g-Whitespace">    </span><span class="mi">758</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">759</span>         <span class="n">data</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span><span class="p">)</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torch\utils\data\_utils\fetch.py:52,</span> in <span class="ni">_MapDatasetFetcher.fetch</span><span class="nt">(self, possibly_batched_index)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>         <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">52</span>         <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span>     <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torch\utils\data\_utils\fetch.py:52,</span> in <span class="ni">&lt;listcomp&gt;</span><span class="nt">(.0)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>         <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">52</span>         <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span>     <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torchvision\datasets\mnist.py:146,</span> in <span class="ni">MNIST.__getitem__</span><span class="nt">(self, index)</span>
<span class="g g-Whitespace">    </span><span class="mi">143</span> <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">145</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">146</span>     <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">148</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">149</span>     <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torchvision\transforms\transforms.py:95,</span> in <span class="ni">Compose.__call__</span><span class="nt">(self, img)</span>
<span class="g g-Whitespace">     </span><span class="mi">93</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span>     <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">95</span>         <span class="n">img</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span>     <span class="k">return</span> <span class="n">img</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torchvision\transforms\transforms.py:137,</span> in <span class="ni">ToTensor.__call__</span><span class="nt">(self, pic)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pic</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span><span class="sd">     Args:</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span><span class="sd">         pic (PIL Image or numpy.ndarray): Image to be converted to tensor.</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">135</span><span class="sd">         Tensor: Converted image.</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">137</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">pic</span><span class="p">)</span>

<span class="nn">File D:\Program Files\Python39\lib\site-packages\torchvision\transforms\functional.py:172,</span> in <span class="ni">to_tensor</span><span class="nt">(pic)</span>
<span class="g g-Whitespace">    </span><span class="mi">170</span> <span class="k">if</span> <span class="n">pic</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">171</span>     <span class="n">img</span> <span class="o">=</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">img</span>
<span class="ne">--&gt; </span><span class="mi">172</span> <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pic</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">F_pil</span><span class="o">.</span><span class="n">get_image_num_channels</span><span class="p">(</span><span class="n">pic</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">173</span> <span class="c1"># put it from HWC to CHW format</span>
<span class="g g-Whitespace">    </span><span class="mi">174</span> <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>经过训练 10 个 epoch，我们得到准确率为 <span class="math notranslate nohighlight">\(97.58\%\)</span>。</p>
</section>
</section>
<section id="id12">
<h2>3.2 卷积神经网络<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>卷积神经网络是包含卷积层的一类特殊的神经网络。 在深度学习中，图像处理的区域检测对象被称为卷积核（convolution kernel）或者滤波器（filter），亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。 当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。 参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据</p>
<p>卷积神经网络具有的特性：</p>
<ol class="arabic simple">
<li><p><strong>平移不变性</strong>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</p></li>
<li><p><strong>局部性</strong>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</p></li>
</ol>
<p>在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。
<img alt="logit" src="../../_images/3_cnn.PNG" /></p>
<section id="id13">
<h3>3.2.1 卷积<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>卷积的数学定义：设两个函数<span class="math notranslate nohighlight">\(f,g:R^d \to R\)</span> 之间的卷积被定义为：
$<span class="math notranslate nohighlight">\(
(f*g)(x)= \int f(z)g(x-z),dz
\)</span><span class="math notranslate nohighlight">\(
也就是说，卷积是当把一个函数“翻转”并移位 \)</span>x<span class="math notranslate nohighlight">\( 时，测量 \)</span>f<span class="math notranslate nohighlight">\( 和 \)</span>g$ 之间的重叠。</p>
<p><strong>图像卷积</strong></p>
<p>在图像中，严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是<strong>互相关运算</strong>（cross-correlation），而不是数学定义上的卷积运算。我们可以把这种运算当作对卷积运算的一种近似，所以我们仍然将这个互相关运算叫做图像卷积。</p>
<p>在卷积层中，输入张量和核张量通过互相关运算产生输出张量。如下图，输入和核函数通过做对应位置的乘法，得到输出 <span class="math notranslate nohighlight">\(19\)</span> 的卷积计算过程为：<span class="math notranslate nohighlight">\(0 \times 0 + 1 \times 1 + 3 \times 2 + 4 \times 3 = 19\)</span>。
<img alt="logit" src="figures/chapter1/3_%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97.png" /></p>
<p>注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于 <span class="math notranslate nohighlight">\(1\)</span>， 而卷积核只与图像中每个大小完全适合的位置进行互相关运算。 所以，输出大小等于等于输入大小 <span class="math notranslate nohighlight">\(n_h \times n_w\)</span> 减去卷积核大小 <span class="math notranslate nohighlight">\(k_h \times k_w\)</span> 得到 <span class="math notranslate nohighlight">\((n_h-k_h+1) \times (n_w-k_w+1)\)</span>。</p>
<p><strong>填充</strong></p>
<p>有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于所导致的。比如，一个 <span class="math notranslate nohighlight">\(240 \times 240\)</span> 像素的图像，经过10层的 <span class="math notranslate nohighlight">\(5 \times 5\)</span> 卷积后，将减少 <span class="math notranslate nohighlight">\(200 \times 200\)</span> 到像素。如此一来，原始图像的边界丢失了许多有用信息。而填充是解决此问题最有效的方法。</p>
<p>填充（padding）：在输入图像的边界填充元素（通常填充元素是 <span class="math notranslate nohighlight">\(0\)</span>）。如下图所示，我们将 <span class="math notranslate nohighlight">\(3 \times 3\)</span> 输入填充到 <span class="math notranslate nohighlight">\(5 \times 5\)</span>，那么它的输出就增加为 <span class="math notranslate nohighlight">\(4 \times 4\)</span>。蓝色阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素: <span class="math notranslate nohighlight">\(0 \times 0+0\times 1+0\times 2+0\times 3 = 0\)</span>。
<img alt="logit" src="../../_images/3_conv.png" /></p>
<p>通常，如果我们添加 <span class="math notranslate nohighlight">\(p_h\)</span> 行填充（大约一半在顶部，一半在底部）和 <span class="math notranslate nohighlight">\(p_w\)</span> 列填充（左侧大约一半，右侧一半），则输出形状将为
$<span class="math notranslate nohighlight">\(
(n_h-k_h+p_h+1) \times (n_w-k_w+p_w+1)
\)</span>$</p>
<p>这意味着输出的高度和宽度将分别增加 <span class="math notranslate nohighlight">\(p_h\)</span> 和 <span class="math notranslate nohighlight">\(p_w\)</span>。</p>
<p>在许多情况下，我们设置 <span class="math notranslate nohighlight">\(p_h=k_h-1\)</span> 和 <span class="math notranslate nohighlight">\(p_w=k_w-1\)</span>，这样就可以使输入和输出具有相同的高度和宽度。假设卷积核的高度 <span class="math notranslate nohighlight">\(k_h\)</span> 是偶数，我们将在上下各填充 <span class="math notranslate nohighlight">\(p_h/2 = (k_h-1)/2\)</span> 行。如果 <span class="math notranslate nohighlight">\(k_h\)</span> 是奇数，则一种可能性是在输入顶部填充 <span class="math notranslate nohighlight">\(\lceil p_h/2\rceil\)</span> 行，在底部填充 <span class="math notranslate nohighlight">\(\lfloor p_h/2\rfloor\)</span> 行。同理，我们填充宽度的两侧。</p>
<p>卷积神经网络中卷积核的高度和宽度通常为奇数，例如 <span class="math notranslate nohighlight">\(1\)</span>、<span class="math notranslate nohighlight">\(3\)</span>、<span class="math notranslate nohighlight">\(5\)</span> 或 <span class="math notranslate nohighlight">\(7\)</span>。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p>
<p>当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。</p>
<p><strong>步幅</strong></p>
<p>有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。步幅则可以在这类情况下提供帮助。在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。</p>
<p>我们将每次滑动元素的数量称为步幅（stride）。</p>
<p>我们只使用过高度或宽度为 <span class="math notranslate nohighlight">\(1\)</span> 的步幅，那么如何使用较大的步幅呢？ 下图是垂直步幅为 <span class="math notranslate nohighlight">\(3\)</span>，水平步幅为 <span class="math notranslate nohighlight">\(2\)</span> 的二维互相关运算。着色部分是输出元素以及用于输出计算的输入和内核张量元素：<span class="math notranslate nohighlight">\(0\times0+6\times1+0\times2+0\times3=6\)</span>。可以看到，为了计算输出中第一列的第二个元素和第一行的第二个元素，卷积窗口分别向下滑动三行和向右滑动两列。但是，当卷积窗口继续向右滑动两列时，没有输出，因为输入元素无法填充窗口（除非我们添加另一列填充）。
<img alt="logit" src="figures/chapter1/3_%E6%AD%A5%E5%B9%85.png" /></p>
<p>通常，当垂直步幅为 <span class="math notranslate nohighlight">\(p_h=k_h-1\)</span>、水平步幅为 <span class="math notranslate nohighlight">\(p_w=k_w-1\)</span> 时，输出形状为
$<span class="math notranslate nohighlight">\(
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.
\)</span>$</p>
<p>如果我们设置了 <span class="math notranslate nohighlight">\(p_h=k_h-1\)</span> 和 <span class="math notranslate nohighlight">\(p_w=k_w-1\)</span>，则输出形状将简化为 <span class="math notranslate nohighlight">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为 <span class="math notranslate nohighlight">\((n_h/s_h) \times (n_w/s_w)\)</span>。</p>
<p><strong>特征映射与感受野</strong></p>
<p>输出的卷积层有时被称为特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素，其感受野（receptive field）是指在前向传播期间可能影响计算的所有元素（来自所有先前层）。更具体来说感受野（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。</p>
<p>卷积神经网络中，越深层的神经元看到的区域越大，如下图所示，卷积核的大小均为 <span class="math notranslate nohighlight">\(3 \times 3\)</span>，步幅均为 <span class="math notranslate nohighlight">\(1\)</span>，绿色标记的是第二层每个神经元看到的区域，黄色标记的是第三层每个神经元看到的区域。具体地，第二层每个神经元可以看到第一层上 <span class="math notranslate nohighlight">\(3 \times 3\)</span> 大小的区域，第三层每个神经元看到第二层上 <span class="math notranslate nohighlight">\(3 \times 3\)</span> 大小的区域，通过该区域又可以看到第一层上 <span class="math notranslate nohighlight">\(5 \times 5\)</span> 大小的区域。
<img alt="logit" src="figures/chapter1/3_%E6%84%9F%E5%8F%97%E9%87%8E.png" /></p>
<p>所以，感受野是个相对概念，某层 feature map上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。多层卷积网络的感受野受到三个参数的影响: 卷积核大小(kernel size) ,填充(padding), 步幅(stride)。请注意，感受野可能大于输入的实际大小，因为卷积运算时必需保证输入核卷积核的大小是一样的，白色虚线为填充。</p>
</section>
<section id="id14">
<h3>3.2.2 池化<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。</p>
<p>而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。此外，当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。而在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。</p>
<p>本节将介绍池化（pooling）层（也称汇聚层），它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。</p>
<p>与卷积层类似，池化层由一个固定形状的窗口组成，每个位置计算一个输出。 该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口遍历的每个位置计算一个输出。不同于卷积层中的输入与卷积核之间的互相关计算，池化层不包含参数。相反，池运算是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为</p>
<ul class="simple">
<li><p>最大池化层（maximum pooling）</p></li>
<li><p>平均池化层（average pooling）</p></li>
</ul>
<p>在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大池化层还是平均池化层。</p>
<p>下图为窗口形状为 <span class="math notranslate nohighlight">\(2\times 2\)</span> 的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素。输出的每一个元素的计算过程是：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
max(0,1,3,4)=4 \\
max(1,2,4,5)=5 \\
max(3,4,6,7)=7 \\
max(4,5,7,8)=8
\end{split}\]</div>
<p><img alt="logit" src="../../_images/3_pooling.png" /></p>
<p>我们可以看出在一个 <span class="math notranslate nohighlight">\(2\times 2\)</span> 的范围内，使用了 <span class="math notranslate nohighlight">\(2\times 2\)</span> 最大池化层，即使在高度或宽度上移动一个元素，网络仍然可以识别到模式。</p>
<p>与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。</p>
</section>
<section id="id15">
<h3>3.2.3 卷积神经网络代码<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<p>下面我们使用 CNN，训练一个手写数字识别模型。</p>
<p>加载数据集</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>

<span class="c1"># Define the transformation to apply to the data</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>

<span class="c1"># Load the MNIST dataset</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="c1"># Set hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Create data loaders</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>定义 CNN 模型，它由两个卷积层、一个池化层和两个全连接层（即MLP）组成。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the CNN model</span>
<span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>选择交叉熵函数作为优化目标，并使用 Adam 作为优化器。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define loss function and optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>模型训练与测试</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Testing</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy on the test set: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch [1/10], Loss: 0.1246
Epoch [2/10], Loss: 0.0387
Epoch [3/10], Loss: 0.0252
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-17-d77f69b9600f&gt;</span> in <span class="ni">&lt;cell line: 2&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span>         <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> 
<span class="ne">----&gt; </span><span class="mi">9</span>         <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>         <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> 

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">&lt;ipython-input-15-e639de4e9030&gt;</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span>         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">16</span>         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span>         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span>         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1735</span>         <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1736</span>             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1737</span> 
<span class="g g-Whitespace">   </span><span class="mi">1738</span>     <span class="c1"># torchrec tests the code consistency with the following code</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1745</span>                 <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1746</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1747</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1748</span> 
<span class="g g-Whitespace">   </span><span class="mi">1749</span>         <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">552</span> 
<span class="g g-Whitespace">    </span><span class="mi">553</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">554</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">555</span> 
<span class="g g-Whitespace">    </span><span class="mi">556</span> 

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py</span> in <span class="ni">_conv_forward</span><span class="nt">(self, input, weight, bias)</span>
<span class="g g-Whitespace">    </span><span class="mi">547</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">548</span>             <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">549</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">550</span>             <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
<span class="g g-Whitespace">    </span><span class="mi">551</span>         <span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id16">
<h2>3.3 循环神经网络<a class="headerlink" href="#id16" title="Link to this heading">#</a></h2>
<p>循环神经网络（Recurrent Neural Network，RNN）是一种在序列数据上进行建模的神经网络模型。与传统的前馈神经网络不同，循环神经网络具有循环连接，可以将前面的信息传递到后面的步骤中，从而捕捉到序列数据中的时序关系。</p>
<section id="id17">
<h3>3.3.1 循环神经网络架构<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>假设我们在时间步 <span class="math notranslate nohighlight">\(t\)</span> 有小批量输入 <span class="math notranslate nohighlight">\(\mathbf{X_t} \in \mathbb{R}^{n \times d}\)</span>。换言之，对于 <span class="math notranslate nohighlight">\(n\)</span> 个序列样本的小批量，<span class="math notranslate nohighlight">\(\mathbf{X_t}\)</span> 的每一行对应于来自该序列的时间步 <span class="math notranslate nohighlight">\(t\)</span> 处的一个样本。接下来，用
<span class="math notranslate nohighlight">\(\mathbf{H_t} \in \mathbb{R}^{n \times h}\)</span> 表示时间步 <span class="math notranslate nohighlight">\(t\)</span> 的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量 <span class="math notranslate nohighlight">\(\mathbf{H_{t-1}}\)</span>， 并引入了一个新的权重参数 <span class="math notranslate nohighlight">\(\mathbf{W_{hh}} \in \mathbb{R}^{h \times h}\)</span>，来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量由当前时间步的输入 与前一个时间步的隐藏变量一起计算得出：
$<span class="math notranslate nohighlight">\(
\mathbf{H_t} = \phi(\mathbf{X_t} \mathbf{W_{xh}} + \mathbf{H_{t-1}} \mathbf{W_{hh}}  + \mathbf{b_h}).
\)</span>$</p>
<p>对于时间步 <span class="math notranslate nohighlight">\(t\)</span>，输出层的输出类似于多层感知机中的计算：
$<span class="math notranslate nohighlight">\(
\mathbf{O_t} = \mathbf{H_t} \mathbf{W_{hq}} + \mathbf{b_q}.
\)</span>$</p>
<p>以上两个公式，可以用下图表示。
<img alt="logit" src="../../_images/3_rnn.png" /></p>
<p>从相邻时间步的隐藏变量 <span class="math notranslate nohighlight">\(\mathbf{H_t}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{H_{t-1}}\)</span> 之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息， 就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为隐状态（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同， 因此这种计算是循环的（recurrent）。于是基于循环计算的隐状态神经网络被命名为循环神经网络（recurrent neural network）。 在循环神经网络中执行计算的层称为循环层（recurrent layer）。</p>
<p>值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。 因此，循环神经网络的参数开销不会随着时间步的增加而增加。</p>
<p>循环神经网络的经典变体包括<strong>长短期记忆网络（Long Short-Term Memory，LSTM）<strong>和</strong>门控循环单元（Gated Recurrent Unit，GRU）</strong>。这些变体通过引入门控机制来解决传统循环神经网络中的梯度消失和梯度爆炸问题，从而改善了模型的长期依赖建模能力。在进行反向传播的时候，我们使用通过时间反向传播（Backpropagation Through Time, BPTT）。它是一种用于训练循环神经网络的反向传播算法。它通过将时间展开的 RNN 视为深度前馈神经网络，并在每个时间步骤上应用标准的反向传播算法来更新模型的权重。</p>
</section>
<section id="id18">
<h3>3.3.2 循环神经网络代码<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>虽然图像是一个二维的，但是如果我们把图像拉伸成一维的向量，那么我们可以把向量的长度当作时间的长度。这样，我们就仍然使用 RNN 模型来做 MNIST 数据集上的手写数字识别。</p>
<p>先定义一些超参数和导入数据。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>

<span class="c1"># Set hyperparameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span>  <span class="c1"># MNIST images are 28x28 pixels</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># Load the MNIST dataset</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="c1"># Create data loaders</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>定义 RNN 模型</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the RNN model</span>
<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>定义损失函数和优化器，并进行训练和测试。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define loss function and optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="n">total_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>

        <span class="c1"># Forward pass</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># Backward and optimize</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">], Step [</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">total_step</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Testing</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">total</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch [1/10], Step [100/938], Loss: 0.9534
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-20-ebe8c0241b9e&gt;</span> in <span class="ni">&lt;cell line: 7&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">total_step</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="ne">----&gt; </span><span class="mi">8</span>     <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span>         <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> 

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py</span> in <span class="ni">__next__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">699</span>                 <span class="c1"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="g g-Whitespace">    </span><span class="mi">700</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>  <span class="c1"># type: ignore[call-arg]</span>
<span class="ne">--&gt; </span><span class="mi">701</span>             <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_data</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">702</span>             <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">    </span><span class="mi">703</span>             <span class="k">if</span> <span class="p">(</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py</span> in <span class="ni">_next_data</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">755</span>     <span class="k">def</span> <span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">756</span>         <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_index</span><span class="p">()</span>  <span class="c1"># may raise StopIteration</span>
<span class="ne">--&gt; </span><span class="mi">757</span>         <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_fetcher</span><span class="o">.</span><span class="n">fetch</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
<span class="g g-Whitespace">    </span><span class="mi">758</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">759</span>             <span class="n">data</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py</span> in <span class="ni">fetch</span><span class="nt">(self, possibly_batched_index)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>                 <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>             <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">52</span>                 <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span>         <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span>             <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py</span> in <span class="ni">&lt;listcomp&gt;</span><span class="nt">(.0)</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span>                 <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>             <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">52</span>                 <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">53</span>         <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span>             <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py</span> in <span class="ni">__getitem__</span><span class="nt">(self, index)</span>
<span class="g g-Whitespace">    </span><span class="mi">144</span> 
<span class="g g-Whitespace">    </span><span class="mi">145</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">146</span>             <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">147</span> 
<span class="g g-Whitespace">    </span><span class="mi">148</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py</span> in <span class="ni">__call__</span><span class="nt">(self, img)</span>
<span class="g g-Whitespace">     </span><span class="mi">93</span>     <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span>         <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">95</span>             <span class="n">img</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span>         <span class="k">return</span> <span class="n">img</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span> 

<span class="nn">/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py</span> in <span class="ni">__call__</span><span class="nt">(self, pic)</span>
<span class="g g-Whitespace">    </span><span class="mi">135</span>             <span class="n">Tensor</span><span class="p">:</span> <span class="n">Converted</span> <span class="n">image</span><span class="o">.</span>
<span class="g g-Whitespace">    </span><span class="mi">136</span>         <span class="s2">&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">137</span><span class="s2">         return F.to_tensor(pic)</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span><span class="s2"> </span>
<span class="g g-Whitespace">    </span><span class="mi">139</span><span class="s2">     def __repr__(self) -&gt; str:</span>

<span class="nn">/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py</span> in <span class="ni">to_tensor</span><span class="nt">(pic)</span>
<span class="g g-Whitespace">    </span><span class="mi">174</span><span class="s2">     img = img.permute((2, 0, 1)).contiguous()</span>
<span class="g g-Whitespace">    </span><span class="mi">175</span><span class="s2">     if isinstance(img, torch.ByteTensor):</span>
<span class="ne">--&gt; </span><span class="mi">176</span><span class="s2">         return img.to(dtype=default_float_dtype).div(255)</span>
<span class="g g-Whitespace">    </span><span class="mi">177</span><span class="s2">     else:</span>
<span class="g g-Whitespace">    </span><span class="mi">178</span><span class="s2">         return img</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>Epoch [1/10], Step [100/938], Loss: 0.9310
Epoch [1/10], Step [200/938], Loss: 0.8129
…
Epoch [10/10], Step [800/938], Loss: 0.0649
Epoch [10/10], Step [900/938], Loss: 0.0684
Test Accuracy: 95.30%</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
</section>
<section id="id19">
<h2>3.4 参考引用<a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<p>[1] Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander, <a class="reference external" href="https://d2l.ai/">动手学深度学习</a></p>
<p>[2] 李宏毅, <a class="reference external" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php">机器学习</a></p>
<p>[3] 包勇军、朱小坤、颜伟鹏、姚普，<a class="reference external" href="http://www.tup.tsinghua.edu.cn/Wap/tsxqy.aspx?id=09165201">图深度学习从理论到实践</a>，清华大学出版社，2022</p>
<p>[4] 马耀、汤继良，<a class="reference external" href="https://item.jd.com/13221338.html">图深度学习（Deep Learning on Graphs 中文版）</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters\chpt1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">欢迎来到《深度学习原理及其应用》</p>
      </div>
    </a>
    <a class="right-next"
       href="../chpt2/Ch1-Object-Detection.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">1. 目标检测简介</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">第一章 深度学习基础</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1 神经网络及其基本组成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.1 神经网络的结构</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">第一章 深度学习基础</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">3.1 神经网络及其基本组成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">3.1.1 神经网络的结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.1.2 输出层与损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.1.3 模型优化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.1.4 模型的过拟合与欠拟合</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.1.5 前向传播和反向传播</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">3.1.6 神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3.2 卷积神经网络</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.2.1 卷积</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.2.2 池化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">3.2.3 卷积神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.3 循环神经网络</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">3.3.1 循环神经网络架构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">3.3.2 循环神经网络代码</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">3.4 参考引用</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://fangli-ying.github.io/">Dr. Fangli Ying</a>
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>